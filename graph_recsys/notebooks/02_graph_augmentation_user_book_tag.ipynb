{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b781313c-5e4d-4d52-a907-625080b8fbdb",
   "metadata": {},
   "source": [
    "# Graph Augmentation for Recommender Systems  \n",
    "## From Pure Collaborative Filtering to Content-Aware Graphs\n",
    "\n",
    "This notebook continues the development of a graph-based recommender system\n",
    "on the **GoodBooks-10k** dataset.\n",
    "\n",
    "In the previous step, we built a **pure collaborative filtering baseline**\n",
    "using a bipartite **user–item graph** and the **LightGCN** model.\n",
    "That experiment established a **reliable performance ceiling** for\n",
    "collaborative signals alone.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Pure collaborative filtering captures **co-consumption patterns**,\n",
    "but ignores **semantic relations between items**.\n",
    "\n",
    "As a result:\n",
    "- sparse items receive little signal,\n",
    "- cold-start behavior is limited,\n",
    "- improvements plateau even with more complex architectures.\n",
    "\n",
    "Instead of increasing model complexity,  \n",
    "we explore a different direction: **graph augmentation**.\n",
    "\n",
    "## Graph Augmentation Idea\n",
    "\n",
    "We enrich the original user–item graph with **content-based nodes**:\n",
    "\n",
    "- **Books** are connected to **tags** (genres, themes, descriptors)\n",
    "- Tags act as **semantic bridges** between otherwise weakly connected items\n",
    "\n",
    "This results in a multi-hop structure:\n",
    "\n",
    "user → book → tag → book\n",
    "\n",
    "Information can now propagate not only through shared users,\n",
    "but also through shared semantic attributes.\n",
    "\n",
    "## Key Principles of This Experiment\n",
    "\n",
    "- **Same data splits** as the CF baseline (no leakage)\n",
    "- **Same training protocol** (loss, sampling, hyperparameters)\n",
    "- **Same evaluation metrics** (Hit@K, NDCG@K)\n",
    "- **Only the graph structure is changed**\n",
    "\n",
    "This ensures a **fair and controlled comparison**.\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Demonstrate that **graph structure matters** more than architectural novelty\n",
    "2. Quantify gains from content-aware message passing\n",
    "3. Build an interpretable and extensible foundation for hybrid GNN recommenders\n",
    "\n",
    "## What This Notebook Is — and Is Not\n",
    "\n",
    "✔ This is a **controlled research step**  \n",
    "✔ This is an **engineering-quality experiment**  \n",
    "✖ This is not a hyperparameter sweep  \n",
    "✖ This is not a SOTA benchmark\n",
    "\n",
    "## Outcome\n",
    "\n",
    "By the end of this notebook, we will compare:\n",
    "\n",
    "- **Pure CF LightGCN**\n",
    "- **Augmented LightGCN (user–book–tag graph)**\n",
    "\n",
    "under identical conditions and evaluate whether\n",
    "semantic graph augmentation leads to meaningful ranking improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb75c31-0809-42de-a167-866cbb2202d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "DATA_PROCESSED: D:\\ML\\GNN\\graph_recsys\\data_processed\\v2_proper\n",
      "ARTIFACTS: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and global configuration\n",
    "# - set seeds, device\n",
    "# - define project paths\n",
    "# - centralize filenames to avoid \"magic strings\"\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import defaultdict\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from torch.optim import Adam\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# Project paths \n",
    "PROJECT_ROOT = Path(r\"D:/ML/GNN/graph_recsys\")  \n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\" / \"v2_proper\"\n",
    "DATA_RAW = PROJECT_ROOT / \"data_raw\"\n",
    "ARTIFACTS = PROJECT_ROOT / \"artifacts\" / \"v2_proper\"\n",
    "\n",
    "for p in [PROJECT_ROOT, DATA_PROCESSED, DATA_RAW, ARTIFACTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"ARTIFACTS:\", ARTIFACTS)\n",
    "\n",
    "# ---- Expected files in v2_proper ----\n",
    "FILES = {\n",
    "    \"train\": \"train_interactions.csv\",\n",
    "    \"val\": \"val_interactions.csv\",\n",
    "    \"test\": \"test_interactions.csv\",\n",
    "    \"user_map\": \"user2idx.csv\",   # columns: user_id, user_idx\n",
    "    \"book_map\": \"book2idx.csv\",   # columns: book_id, book_idx\n",
    "}\n",
    "\n",
    "# ---- Raw content files ----\n",
    "RAW_FILES = {\n",
    "    \"tags\": \"tags.csv\",           # tag_id, tag_name\n",
    "    \"book_tags\": \"book_tags.csv\", # goodbooks format: goodreads_book_id, tag_id, count\n",
    "}\n",
    "\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ea81d0-89e1-401e-b00f-fe519c063f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ui: (4926384, 2) int32\n",
      "val_ui: (53398, 2) int32\n",
      "test_ui: (53398, 2) int32\n",
      "n_users: 53398 n_items: 9999\n",
      "num_users(from mapping): 53398 num_items(from mapping): 9999\n",
      "train_df: (4926384, 4) val_df: (53398, 4) test_df: (53398, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>book_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1796</td>\n",
       "      <td>0</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4691</td>\n",
       "      <td>0</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2063</td>\n",
       "      <td>0</td>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  user_idx  book_idx\n",
       "0        1      258         0       257\n",
       "1        1     1796         0      1795\n",
       "2        1     4691         0      4690\n",
       "3        1     2063         0      2062\n",
       "4        1       11         0        10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load splits and mappings (your actual saved format)\n",
    "# - splits_ui.npz contains train_ui/val_ui/test_ui arrays of shape (N,2) with (u,i) indices\n",
    "# - user2idx.csv and book2idx.csv were saved via pd.Series(...).to_csv()\n",
    "#   => CSV has two columns: [index, value] i.e. [id, idx] but without headers\n",
    "\n",
    "def load_series_mapping(path: Path) -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Loads mapping saved by:\n",
    "        pd.Series(mapping_dict).to_csv(path)\n",
    "    The resulting CSV usually has columns like: ['Unnamed: 0', '0'].\n",
    "    Returns: {original_id: mapped_idx}\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)  # will have 2 columns: index + value\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        # if something really weird happened\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        if df.shape[1] < 2:\n",
    "            raise ValueError(f\"Mapping file {path} must have 2 columns (id, idx). Got: {df.shape}\")\n",
    "\n",
    "    id_col = df.columns[0]\n",
    "    idx_col = df.columns[1]\n",
    "\n",
    "    ids = pd.to_numeric(df[id_col], errors=\"raise\").astype(int).to_numpy()\n",
    "    idx = pd.to_numeric(df[idx_col], errors=\"raise\").astype(int).to_numpy()\n",
    "\n",
    "    mapping = dict(zip(ids, idx))\n",
    "    return mapping\n",
    "\n",
    "# Paths\n",
    "splits_path   = DATA_PROCESSED / \"splits_ui.npz\"\n",
    "user_map_path = DATA_PROCESSED / \"user2idx.csv\"\n",
    "book_map_path = DATA_PROCESSED / \"book2idx.csv\"\n",
    "\n",
    "for p in [splits_path, user_map_path, book_map_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "# Load splits\n",
    "z = np.load(splits_path, allow_pickle=True)\n",
    "assert \"train_ui\" in z and \"val_ui\" in z and \"test_ui\" in z, f\"Unexpected keys in splits: {list(z.keys())}\"\n",
    "\n",
    "train_ui = z[\"train_ui\"].astype(np.int32)\n",
    "val_ui   = z[\"val_ui\"].astype(np.int32)\n",
    "test_ui  = z[\"test_ui\"].astype(np.int32)\n",
    "\n",
    "n_users = int(z[\"n_users\"]) if \"n_users\" in z else None\n",
    "n_items = int(z[\"n_items\"]) if \"n_items\" in z else None\n",
    "\n",
    "print(\"train_ui:\", train_ui.shape, train_ui.dtype)\n",
    "print(\"val_ui:\", val_ui.shape, val_ui.dtype)\n",
    "print(\"test_ui:\", test_ui.shape, test_ui.dtype)\n",
    "print(\"n_users:\", n_users, \"n_items:\", n_items)\n",
    "\n",
    "assert train_ui.ndim == 2 and train_ui.shape[1] == 2, f\"train_ui must be (N,2), got {train_ui.shape}\"\n",
    "assert val_ui.ndim == 2 and val_ui.shape[1] == 2, f\"val_ui must be (N,2), got {val_ui.shape}\"\n",
    "assert test_ui.ndim == 2 and test_ui.shape[1] == 2, f\"test_ui must be (N,2), got {test_ui.shape}\"\n",
    "\n",
    "# Load mappings (Series to CSV format)\n",
    "user2idx = load_series_mapping(user_map_path)   # {user_id: u}\n",
    "book2idx = load_series_mapping(book_map_path)   # {book_id: i}\n",
    "\n",
    "# Build inverse mappings (idx -> original id) for readability/debug\n",
    "# Important: this works because ids were saved as keys in the Series\n",
    "idx2user = {u: user_id for user_id, u in user2idx.items()}\n",
    "idx2book = {i: book_id for book_id, i in book2idx.items()}\n",
    "\n",
    "# Infer counts from mapping (more reliable than npz if mismatch)\n",
    "num_users = max(idx2user.keys()) + 1 if len(idx2user) else 0\n",
    "num_items = max(idx2book.keys()) + 1 if len(idx2book) else 0\n",
    "\n",
    "print(\"num_users(from mapping):\", num_users, \"num_items(from mapping):\", num_items)\n",
    "\n",
    "# Optional: consistency check with npz counts\n",
    "if n_users is not None:\n",
    "    assert n_users == num_users, f\"Mismatch n_users: npz={n_users}, mapping={num_users}\"\n",
    "if n_items is not None:\n",
    "    assert n_items == num_items, f\"Mismatch n_items: npz={n_items}, mapping={num_items}\"\n",
    "\n",
    "# Build DataFrames for convenience (both idx and original ids)\n",
    "def ui_to_df(ui: np.ndarray, name: str) -> pd.DataFrame:\n",
    "    u = ui[:, 0].astype(int)\n",
    "    i = ui[:, 1].astype(int)\n",
    "    df = pd.DataFrame({\"user_idx\": u, \"book_idx\": i})\n",
    "    df[\"user_id\"] = df[\"user_idx\"].map(idx2user).astype(int)\n",
    "    df[\"book_id\"] = df[\"book_idx\"].map(idx2book).astype(int)\n",
    "    return df[[\"user_id\", \"book_id\", \"user_idx\", \"book_idx\"]]\n",
    "\n",
    "train_df = ui_to_df(train_ui, \"train\")\n",
    "val_df   = ui_to_df(val_ui, \"val\")\n",
    "test_df  = ui_to_df(test_ui, \"test\")\n",
    "\n",
    "print(\"train_df:\", train_df.shape, \"val_df:\", val_df.shape, \"test_df:\", test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e9bda-2cef-42f8-95f2-f20859d24127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags_df: (34252, 2) | columns: ['tag_id', 'tag_name']\n",
      "book_tags_df: (999912, 3) | columns: ['goodreads_book_id', 'tag_id', 'count']\n",
      "book_tags dtypes: {'goodreads_book_id': dtype('int64'), 'tag_id': dtype('int64'), 'count': dtype('int64')}\n",
      "book_tags_df filtered to mapped books: (81200, 3)\n",
      "unique books with tags (raw, before any filtering): 812\n",
      "unique tags in relations (raw, before filtering): 6734\n"
     ]
    }
   ],
   "source": [
    "# Cell 3a: Load raw tags relations (tags.csv, book_tags.csv)\n",
    "# - читаем raw файлы из data_raw\n",
    "# - оставляем только книги, которые присутствуют в нашем book2idx (9999 книг)\n",
    "# - приводим типы, проверяем схему колонок\n",
    "\n",
    "def read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "tags_path = DATA_RAW / RAW_FILES[\"tags\"]\n",
    "book_tags_path = DATA_RAW / RAW_FILES[\"book_tags\"]\n",
    "\n",
    "tags_df = read_csv(tags_path)\n",
    "book_tags_df = read_csv(book_tags_path)\n",
    "\n",
    "print(\"tags_df:\", tags_df.shape, \"| columns:\", tags_df.columns.tolist())\n",
    "print(\"book_tags_df:\", book_tags_df.shape, \"| columns:\", book_tags_df.columns.tolist())\n",
    "print(\"book_tags dtypes:\", book_tags_df.dtypes.to_dict())\n",
    "\n",
    "# schema normalization (defensive)\n",
    "rename_map = {}\n",
    "if \"goodreads_book_id\" not in book_tags_df.columns and \"book_id\" in book_tags_df.columns:\n",
    "    rename_map[\"book_id\"] = \"goodreads_book_id\"\n",
    "if rename_map:\n",
    "    book_tags_df = book_tags_df.rename(columns=rename_map)\n",
    "\n",
    "assert {\"goodreads_book_id\", \"tag_id\"}.issubset(book_tags_df.columns), \\\n",
    "    \"book_tags.csv must contain goodreads_book_id and tag_id\"\n",
    "assert {\"tag_id\"}.issubset(tags_df.columns), \"tags.csv must contain tag_id\"\n",
    "assert \"count\" in book_tags_df.columns, \"Expected 'count' in book_tags.csv\"\n",
    "\n",
    "# types\n",
    "book_tags_df[\"goodreads_book_id\"] = book_tags_df[\"goodreads_book_id\"].astype(int)\n",
    "book_tags_df[\"tag_id\"] = book_tags_df[\"tag_id\"].astype(int)\n",
    "book_tags_df[\"count\"] = book_tags_df[\"count\"].astype(int)\n",
    "\n",
    "# IMPORTANT: keep only books that exist in our mapping (book2idx keys are original book_id)\n",
    "book_ids_in_mapping = set(book2idx.keys())\n",
    "book_tags_df = book_tags_df[book_tags_df[\"goodreads_book_id\"].isin(book_ids_in_mapping)].copy()\n",
    "\n",
    "print(\"book_tags_df filtered to mapped books:\", book_tags_df.shape)\n",
    "print(\"unique books with tags (raw, before any filtering):\", book_tags_df[\"goodreads_book_id\"].nunique())\n",
    "print(\"unique tags in relations (raw, before filtering):\", book_tags_df[\"tag_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d747e7-0be3-48b9-b0cc-8c45fae23d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train∩val: 0\n",
      "train∩test: 0\n",
      "val∩test: 0\n",
      "Splits are consistent ✔\n"
     ]
    }
   ],
   "source": [
    "# Cell 3b: Leakage and index-range checks (in u/i space)\n",
    "\n",
    "def check_split(df: pd.DataFrame, name: str, n_users: int, n_items: int) -> None:\n",
    "    assert df.user_idx.min() >= 0 and df.user_idx.max() < n_users, f\"{name}: user_idx out of range\"\n",
    "    assert df.book_idx.min() >= 0 and df.book_idx.max() < n_items, f\"{name}: book_idx out of range\"\n",
    "\n",
    "check_split(train_df, \"train\", num_users, num_items)\n",
    "check_split(val_df, \"val\", num_users, num_items)\n",
    "check_split(test_df, \"test\", num_users, num_items)\n",
    "\n",
    "train_pairs = set(zip(train_df.user_idx, train_df.book_idx))\n",
    "val_pairs   = set(zip(val_df.user_idx, val_df.book_idx))\n",
    "test_pairs  = set(zip(test_df.user_idx, test_df.book_idx))\n",
    "\n",
    "print(\"train∩val:\", len(train_pairs & val_pairs))\n",
    "print(\"train∩test:\", len(train_pairs & test_pairs))\n",
    "print(\"val∩test:\", len(val_pairs & test_pairs))\n",
    "\n",
    "assert len(train_pairs & val_pairs) == 0, \"Leakage: train overlaps val\"\n",
    "assert len(train_pairs & test_pairs) == 0, \"Leakage: train overlaps test\"\n",
    "assert len(val_pairs & test_pairs) == 0, \"Leakage: val overlaps test\"\n",
    "\n",
    "print(\"Splits are consistent ✔\")\n",
    "\n",
    "train_u = train_df[\"user_idx\"].to_numpy()\n",
    "train_b = train_df[\"book_idx\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecb1484-17f4-4fb7-803a-33dcd466b94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline reference metrics:\n",
      "  model: LightGCN (user–book) + hard negatives\n",
      "  Hit@10: 0.084\n",
      "  NDCG@10: 0.045\n",
      "  Hit@20: 0.129\n",
      "  NDCG@20: 0.057\n",
      "  Hit@50: 0.221\n",
      "  NDCG@50: 0.075\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Baseline reference metrics (from Notebook 02 / handoff)\n",
    "# - we didn't persist baseline_metrics.json, so we store the key numbers here\n",
    "# - these values are used only for comparison tables (no training/eval uses them)\n",
    "\n",
    "baseline_ref = {\n",
    "    \"model\": \"LightGCN (user–book) + hard negatives\",\n",
    "    \"Hit@10\": 0.084,\n",
    "    \"NDCG@10\": 0.045,\n",
    "    \"Hit@20\": 0.129,\n",
    "    \"NDCG@20\": 0.057,\n",
    "    \"Hit@50\": 0.221,\n",
    "    \"NDCG@50\": 0.075,\n",
    "}\n",
    "\n",
    "print(\"Baseline reference metrics:\")\n",
    "for k, v in baseline_ref.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6049ae2-578e-43e4-b2ed-3ac72694fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags_df: (34252, 2) | columns: ['tag_id', 'tag_name']\n",
      "book_tags raw: (999912, 3) | dtypes: {'goodreads_book_id': dtype('int32'), 'tag_id': dtype('int32'), 'count': dtype('int32')}\n",
      "book_tags filtered to mapped books: (81200, 3)\n",
      "book_tags aggregated: (81199, 3)\n",
      "unique books with tags: 812\n",
      "unique tags in relations: 6734\n",
      "tags per book: mean= 99.9987684729064 median= 100.0 p90= 100.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load content relations (book-tags) in a RAM-safe way\n",
    "# - read only needed columns\n",
    "# - enforce dtypes to reduce memory\n",
    "# - filter to our book_id universe early\n",
    "# - aggregate duplicates: (book_id, tag_id) -> count_sum\n",
    "\n",
    "# Paths\n",
    "tags_path = DATA_RAW / RAW_FILES[\"tags\"]\n",
    "book_tags_path = DATA_RAW / RAW_FILES[\"book_tags\"]\n",
    "\n",
    "if not tags_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing file: {tags_path}\")\n",
    "if not book_tags_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing file: {book_tags_path}\")\n",
    "\n",
    "# ---- tags.csv (small) ----\n",
    "# GoodBooks: tag_id, tag_name\n",
    "tags_df = pd.read_csv(tags_path)\n",
    "assert \"tag_id\" in tags_df.columns, f\"tags.csv must contain tag_id, got {tags_df.columns}\"\n",
    "tag_name_col = \"tag_name\" if \"tag_name\" in tags_df.columns else None\n",
    "\n",
    "print(\"tags_df:\", tags_df.shape, \"| columns:\", tags_df.columns.tolist())\n",
    "\n",
    "# ---- book_tags.csv (large) ----\n",
    "# GoodBooks: goodreads_book_id, tag_id, count\n",
    "# Load only necessary columns with small dtypes\n",
    "usecols = None  # let pandas infer cols then subselect if schema differs\n",
    "bt = pd.read_csv(\n",
    "    book_tags_path,\n",
    "    usecols=[\"goodreads_book_id\", \"tag_id\", \"count\"],  \n",
    "    dtype={\"goodreads_book_id\": np.int32, \"tag_id\": np.int32, \"count\": np.int32},\n",
    ")\n",
    "\n",
    "print(\"book_tags raw:\", bt.shape, \"| dtypes:\", bt.dtypes.to_dict())\n",
    "\n",
    "# Filter to books that exist in our mapping (early!)\n",
    "# book2idx is {book_id(original): book_idx}\n",
    "bt = bt[bt[\"goodreads_book_id\"].isin(book2idx)].copy()\n",
    "print(\"book_tags filtered to mapped books:\", bt.shape)\n",
    "\n",
    "# Aggregate duplicates: for graph we want one edge per (book,tag)\n",
    "bt_agg = (\n",
    "    bt.groupby([\"goodreads_book_id\", \"tag_id\"], as_index=False)[\"count\"]\n",
    "      .sum()\n",
    "      .rename(columns={\"goodreads_book_id\": \"book_id\"})\n",
    ")\n",
    "\n",
    "print(\"book_tags aggregated:\", bt_agg.shape)\n",
    "print(\"unique books with tags:\", bt_agg[\"book_id\"].nunique())\n",
    "print(\"unique tags in relations:\", bt_agg[\"tag_id\"].nunique())\n",
    "\n",
    "# Quick stats\n",
    "tags_per_book = bt_agg.groupby(\"book_id\")[\"tag_id\"].nunique()\n",
    "print(\"tags per book: mean=\", float(tags_per_book.mean()),\n",
    "      \"median=\", float(tags_per_book.median()),\n",
    "      \"p90=\", float(tags_per_book.quantile(0.9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb61a21-5b26-4963-bfbd-401c37005f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After MIN_BOOK_FREQ: (55262, 3) | tags: 307\n",
      "After TOP_TAGS_PER_BOOK: (16239, 3) | tags: 265\n",
      "Filtered tags/book: mean= 19.998768472906406 median= 20.0 p90= 20.0\n",
      "Final tag vocabulary size T: 265\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>count</th>\n",
       "      <th>book_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>30574</td>\n",
       "      <td>167697</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>11305</td>\n",
       "      <td>37174</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>11557</td>\n",
       "      <td>34173</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>8717</td>\n",
       "      <td>12986</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>33114</td>\n",
       "      <td>12716</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_id  tag_id   count  book_idx  tag_idx\n",
       "90        1   30574  167697         0      244\n",
       "31        1   11305   37174         0      105\n",
       "37        1   11557   34173         0      112\n",
       "26        1    8717   12986         0       86\n",
       "97        1   33114   12716         0      262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: Filter tags to control noise/density + convert to idx-space\n",
    "# - filter tags by min book frequency\n",
    "# - optionally keep global TOP_K tags\n",
    "# - keep TOP_TAGS_PER_BOOK per book by count\n",
    "# - output: filtered_bt with [book_id, tag_id, count, book_idx, tag_idx]\n",
    "\n",
    "MIN_BOOK_FREQ = 50        # tag must appear in >= this many books\n",
    "TOP_K_TAGS = None         # e.g. 5000, or None\n",
    "TOP_TAGS_PER_BOOK = 20    # keep top tags per book by count\n",
    "\n",
    "# Tag frequency across books\n",
    "tag_book_freq = (\n",
    "    bt_agg.groupby(\"tag_id\")[\"book_id\"]\n",
    "          .nunique()\n",
    "          .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "eligible_tags = tag_book_freq[tag_book_freq >= MIN_BOOK_FREQ].index\n",
    "filtered_bt = bt_agg[bt_agg[\"tag_id\"].isin(eligible_tags)].copy()\n",
    "print(\"After MIN_BOOK_FREQ:\", filtered_bt.shape, \"| tags:\", filtered_bt[\"tag_id\"].nunique())\n",
    "\n",
    "if TOP_K_TAGS is not None:\n",
    "    top_tags = tag_book_freq.loc[eligible_tags].head(TOP_K_TAGS).index\n",
    "    filtered_bt = filtered_bt[filtered_bt[\"tag_id\"].isin(top_tags)].copy()\n",
    "    print(\"After TOP_K_TAGS:\", filtered_bt.shape, \"| tags:\", filtered_bt[\"tag_id\"].nunique())\n",
    "\n",
    "# Keep top tags per book by count\n",
    "filtered_bt = (\n",
    "    filtered_bt.sort_values([\"book_id\", \"count\"], ascending=[True, False])\n",
    "               .groupby(\"book_id\", as_index=False)\n",
    "               .head(TOP_TAGS_PER_BOOK)\n",
    "               .copy()\n",
    ")\n",
    "\n",
    "print(\"After TOP_TAGS_PER_BOOK:\", filtered_bt.shape, \"| tags:\", filtered_bt[\"tag_id\"].nunique())\n",
    "\n",
    "# Convert to idx-space for graph building\n",
    "filtered_bt[\"book_idx\"] = filtered_bt[\"book_id\"].map(book2idx).astype(np.int32)\n",
    "\n",
    "# Create tag2idx based on filtered tags set\n",
    "unique_tag_ids = np.sort(filtered_bt[\"tag_id\"].unique())\n",
    "tag2idx_local = {int(tid): i for i, tid in enumerate(unique_tag_ids)}\n",
    "filtered_bt[\"tag_idx\"] = filtered_bt[\"tag_id\"].map(tag2idx_local).astype(np.int32)\n",
    "\n",
    "# Stats after filtering\n",
    "filtered_tags_per_book = filtered_bt.groupby(\"book_id\")[\"tag_id\"].nunique()\n",
    "print(\"Filtered tags/book: mean=\", float(filtered_tags_per_book.mean()),\n",
    "      \"median=\", float(filtered_tags_per_book.median()),\n",
    "      \"p90=\", float(filtered_tags_per_book.quantile(0.9)))\n",
    "\n",
    "T = len(tag2idx_local)\n",
    "print(\"Final tag vocabulary size T:\", T)\n",
    "\n",
    "filtered_bt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058f9f2d-530e-4410-9a4f-b709e0a892b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=53398 | B=9999 | T=265 | num_nodes=63662\n",
      "Offsets: {'user': 0, 'book': np.int64(53398), 'tag': np.int64(63397)}\n",
      "Unified indexing ready ✔\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Unified node index space (users + books + tags)\n",
    "# - users: [0 .. U)\n",
    "# - books: [U .. U+B)\n",
    "# - tags : [U+B .. U+B+T)\n",
    "\n",
    "U = num_users\n",
    "B = num_items\n",
    "T = len(tag2idx_local)\n",
    "\n",
    "user_offset = 0\n",
    "book_offset = U\n",
    "tag_offset  = U + B\n",
    "num_nodes   = U + B + T\n",
    "\n",
    "print(f\"U={U} | B={B} | T={T} | num_nodes={num_nodes}\")\n",
    "print(\"Offsets:\", {\"user\": user_offset, \"book\": book_offset, \"tag\": tag_offset})\n",
    "print(\"Unified indexing ready ✔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c2d153-b019-448b-a39e-bb52a991e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User–Book edges (directed count): 9852768\n",
      "Unique TRAIN interactions: 4926384\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Build user–book edges from TRAIN interactions only (idx-space)\n",
    "# - no val/test edges (avoid leakage)\n",
    "# - bidirectional edges (undirected graph)\n",
    "\n",
    "train_u = train_df[\"user_idx\"].to_numpy(dtype=np.int64)\n",
    "train_b = train_df[\"book_idx\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "src_ub = torch.from_numpy(train_u + user_offset).long()\n",
    "dst_ub = torch.from_numpy(train_b + book_offset).long()\n",
    "\n",
    "edge_src = torch.cat([src_ub, dst_ub], dim=0)\n",
    "edge_dst = torch.cat([dst_ub, src_ub], dim=0)\n",
    "\n",
    "print(\"User–Book edges (directed count):\", edge_src.numel())\n",
    "print(\"Unique TRAIN interactions:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b6524b-529d-4c82-bf21-a881fe691494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges after adding book–tag (directed count): 9885246\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Build book–tag edges (book ↔ tag)\n",
    "# - uses filtered_bt with columns: book_idx, tag_idx\n",
    "# - bidirectional edges\n",
    "\n",
    "bt_books = filtered_bt[\"book_idx\"].to_numpy(dtype=np.int64)\n",
    "bt_tags  = filtered_bt[\"tag_idx\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "src_bt = torch.from_numpy(bt_books + book_offset).long()\n",
    "dst_bt = torch.from_numpy(bt_tags  + tag_offset).long()\n",
    "\n",
    "edge_src = torch.cat([edge_src, src_bt, dst_bt], dim=0)\n",
    "edge_dst = torch.cat([edge_dst, dst_bt, src_bt], dim=0)\n",
    "\n",
    "print(\"Total edges after adding book–tag (directed count):\", edge_src.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e710c4a0-efbf-4c66-b2d1-0470e0f3a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index shape: torch.Size([2, 9885246])\n",
      "num_nodes: 63662\n",
      "Graph build sanity ✔\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Combine edges into edge_index (+ optional weights)\n",
    "# - edge_index: [2, E]\n",
    "# - for now: uniform edge_weight=1\n",
    "\n",
    "edge_index = torch.stack([edge_src, edge_dst], dim=0)\n",
    "edge_weight = torch.ones(edge_index.size(1), dtype=torch.float32)\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "print(\"num_nodes:\", num_nodes)\n",
    "\n",
    "assert edge_index.min().item() >= 0\n",
    "assert edge_index.max().item() < num_nodes\n",
    "print(\"Graph build sanity ✔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d9077c-b570-4652-aca3-85c24de41244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm: torch.Size([63662, 63662]) nnz: 9885246\n",
      "deg stats: 1.0 155.2770233154297 19472.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Build normalized adjacency for LightGCN\n",
    "# - create sparse adjacency A (undirected)\n",
    "# - apply LightGCN normalization: D^{-1/2} A D^{-1/2}\n",
    "# - store as torch.sparse_coo_tensor on DEVICE\n",
    "\n",
    "import torch\n",
    "\n",
    "E = edge_index.size(1)\n",
    "idx = edge_index.long()\n",
    "\n",
    "# values are 1 for all edges\n",
    "val = torch.ones(E, dtype=torch.float32)\n",
    "\n",
    "A = torch.sparse_coo_tensor(idx, val, (num_nodes, num_nodes)).coalesce()\n",
    "\n",
    "deg = torch.sparse.sum(A, dim=1).to_dense()  # [N]\n",
    "deg_inv_sqrt = torch.pow(deg.clamp(min=1.0), -0.5)\n",
    "\n",
    "# normalized values: v_ij = 1 / sqrt(deg_i * deg_j)\n",
    "row, col = A.indices()\n",
    "norm_val = deg_inv_sqrt[row] * A.values() * deg_inv_sqrt[col]\n",
    "\n",
    "A_norm = torch.sparse_coo_tensor(\n",
    "    A.indices(), norm_val, A.size()\n",
    ").coalesce().to(DEVICE)\n",
    "\n",
    "print(\"A_norm:\", A_norm.shape, \"nnz:\", A_norm._nnz())\n",
    "print(\"deg stats:\", float(deg.min()), float(deg.mean()), float(deg.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec1e790f-48da-4fdf-b388-a14a7f244890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm values: min/mean/max = 0.00025242200354114175 0.003876851871609688 0.09622504562139511\n",
      "NaN in values: False | Inf in values: False\n",
      "propagate output: torch.Size([63662, 64]) NaN: False Inf: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 11b: Sanity check for normalization\n",
    "# - ensure no NaN/Inf in A_norm values\n",
    "# - run one sparse mm to verify it works on DEVICE\n",
    "\n",
    "vals = A_norm.values()\n",
    "print(\"A_norm values: min/mean/max =\", float(vals.min()), float(vals.mean()), float(vals.max()))\n",
    "print(\"NaN in values:\", torch.isnan(vals).any().item(), \"| Inf in values:\", torch.isinf(vals).any().item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.emb.weight\n",
    "    y = torch.sparse.mm(A_norm, x)\n",
    "    print(\"propagate output:\", y.shape, \"NaN:\", torch.isnan(y).any().item(), \"Inf:\", torch.isinf(y).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20096d52-aafa-457d-bf9e-fe89122ea519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGCN(\n",
      "  (emb): Embedding(63662, 64)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define LightGCN model\n",
    "# - learn embeddings for all nodes (users+books+tags)\n",
    "# - propagate K layers using normalized adjacency\n",
    "# - final embedding is mean of layer-wise embeddings\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def propagate(self, A_norm: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = self.emb.weight\n",
    "        xs = [x0]\n",
    "        x = x0\n",
    "        for _ in range(self.num_layers):\n",
    "            x = torch.sparse.mm(A_norm, x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "        x_out = torch.stack(xs, dim=0).mean(dim=0)  # [N, D]\n",
    "        return x_out\n",
    "\n",
    "model = LightGCN(num_nodes=num_nodes, emb_dim=64, num_layers=3, dropout=0.0).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a5c208f-1015-45d4-b150-7e2db2e9d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos users: 53398\n",
      "val_gt users: 53398 test_gt users: 53398\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Build user->positive sets\n",
    "# - train_pos: for negative sampling (avoid sampling seen positives)\n",
    "# - val_gt/test_gt: ground-truth one item per user for evaluation (leave-one-out)\n",
    "\n",
    "train_pos = defaultdict(set)\n",
    "for u, i in zip(train_df[\"user_idx\"].to_numpy(), train_df[\"book_idx\"].to_numpy()):\n",
    "    train_pos[int(u)].add(int(i))\n",
    "\n",
    "# Ground truth: exactly 1 item per user (by construction)\n",
    "val_gt = val_df.set_index(\"user_idx\")[\"book_idx\"].to_dict()\n",
    "test_gt = test_df.set_index(\"user_idx\")[\"book_idx\"].to_dict()\n",
    "\n",
    "print(\"train_pos users:\", len(train_pos))\n",
    "print(\"val_gt users:\", len(val_gt), \"test_gt users:\", len(test_gt))\n",
    "\n",
    "# sanity: each user should exist in val/test\n",
    "assert len(val_gt) == num_users and len(test_gt) == num_users, \"Expected 1 val and 1 test item per user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca74c456-f03b-4bfe-a210-f2abef99ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval function ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Ranking evaluation (leave-one-out) on validation\n",
    "# - compute scores over ALL items (B=9999) batched by users\n",
    "# - filter already-seen train positives\n",
    "# - metrics: Hit@K, NDCG@K\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ranking(all_emb: torch.Tensor,\n",
    "                 gt: dict[int, int],\n",
    "                 train_pos: dict[int, set],\n",
    "                 K_list=(10, 20, 50),\n",
    "                 user_batch_size: int = 512) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    all_emb: [N, D] final node embeddings\n",
    "    gt: {user_idx: true_item_idx}\n",
    "    train_pos: {user_idx: set(seen_item_idx)} used for filtering\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Extract user and item embeddings (books only)\n",
    "    user_emb = all_emb[user_offset:user_offset + U]          # [U, D]\n",
    "    item_emb = all_emb[book_offset:book_offset + B]          # [B, D]\n",
    "\n",
    "    # Prepare tensors on DEVICE\n",
    "    user_emb = user_emb.to(DEVICE)\n",
    "    item_emb = item_emb.to(DEVICE)\n",
    "\n",
    "    users = np.array(sorted(gt.keys()), dtype=np.int32)\n",
    "    hits = {K: 0 for K in K_list}\n",
    "    ndcgs = {K: 0.0 for K in K_list}\n",
    "\n",
    "    for start in range(0, len(users), user_batch_size):\n",
    "        batch_users = users[start:start + user_batch_size]\n",
    "        bu = torch.from_numpy(batch_users.astype(np.int64)).to(DEVICE)\n",
    "\n",
    "        # scores: [batch, B]\n",
    "        scores = user_emb[bu] @ item_emb.t()\n",
    "\n",
    "        # filter training positives: set their scores to -inf\n",
    "        for row, u in enumerate(batch_users):\n",
    "            seen = train_pos.get(int(u), None)\n",
    "            if seen:\n",
    "                seen_idx = torch.tensor(list(seen), device=DEVICE, dtype=torch.long)\n",
    "                scores[row, seen_idx] = -1e9\n",
    "\n",
    "        # get top maxK\n",
    "        maxK = max(K_list)\n",
    "        topk_items = torch.topk(scores, k=maxK, dim=1).indices.cpu().numpy()  # [batch, maxK]\n",
    "\n",
    "        for row, u in enumerate(batch_users):\n",
    "            true_i = int(gt[int(u)])\n",
    "            ranking = topk_items[row]  # top maxK item indices\n",
    "\n",
    "            # find rank position if present\n",
    "            # rank is 1-based\n",
    "            pos = np.where(ranking == true_i)[0]\n",
    "            for K in K_list:\n",
    "                if len(pos) > 0 and pos[0] < K:\n",
    "                    hits[K] += 1\n",
    "                    ndcgs[K] += 1.0 / np.log2(pos[0] + 2.0)\n",
    "\n",
    "    n = len(users)\n",
    "    metrics = {}\n",
    "    for K in K_list:\n",
    "        metrics[f\"Hit@{K}\"] = hits[K] / n\n",
    "        metrics[f\"NDCG@{K}\"] = ndcgs[K] / n\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"Eval function ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c2dddaf-511f-4225-8807-bfcbe53800a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config: {'EMB_DIM': 64, 'NUM_LAYERS': 3, 'LR': 0.002, 'WEIGHT_DECAY': 0.0, 'BATCH_SIZE': 4096, 'ACC_STEPS': 16, 'EPOCHS': 20, 'USER_BATCH_EVAL': 1024, 'PATIENCE': 4, 'MIN_DELTA': 1e-05}\n",
      "Epoch 01 | loss=0.5740 | steps=76 | time=35.1s | Hit@10=0.0462 NDCG@10=0.0262 | Hit@50=0.1294 NDCG@50=0.0442\n",
      "Best NDCG@10 so far: 0.02619 (epoch 1), patience=0/4\n",
      "Epoch 02 | loss=0.4058 | steps=76 | time=36.6s | Hit@10=0.0488 NDCG@10=0.0273 | Hit@50=0.1345 NDCG@50=0.0458\n",
      "Best NDCG@10 so far: 0.02729 (epoch 2), patience=0/4\n",
      "Epoch 03 | loss=0.3694 | steps=76 | time=35.2s | Hit@10=0.0548 NDCG@10=0.0297 | Hit@50=0.1434 NDCG@50=0.0487\n",
      "Best NDCG@10 so far: 0.02968 (epoch 3), patience=0/4\n",
      "Epoch 04 | loss=0.3368 | steps=76 | time=34.9s | Hit@10=0.0583 NDCG@10=0.0311 | Hit@50=0.1515 NDCG@50=0.0511\n",
      "Best NDCG@10 so far: 0.03115 (epoch 4), patience=0/4\n",
      "Epoch 05 | loss=0.3149 | steps=76 | time=35.6s | Hit@10=0.0596 NDCG@10=0.0318 | Hit@50=0.1564 NDCG@50=0.0526\n",
      "Best NDCG@10 so far: 0.03184 (epoch 5), patience=0/4\n",
      "Epoch 06 | loss=0.2966 | steps=76 | time=35.7s | Hit@10=0.0613 NDCG@10=0.0329 | Hit@50=0.1625 NDCG@50=0.0545\n",
      "Best NDCG@10 so far: 0.03287 (epoch 6), patience=0/4\n",
      "Epoch 07 | loss=0.2776 | steps=76 | time=35.6s | Hit@10=0.0639 NDCG@10=0.0344 | Hit@50=0.1700 NDCG@50=0.0571\n",
      "Best NDCG@10 so far: 0.03442 (epoch 7), patience=0/4\n",
      "Epoch 08 | loss=0.2580 | steps=76 | time=35.5s | Hit@10=0.0679 NDCG@10=0.0364 | Hit@50=0.1795 NDCG@50=0.0603\n",
      "Best NDCG@10 so far: 0.03642 (epoch 8), patience=0/4\n",
      "Epoch 09 | loss=0.2420 | steps=76 | time=36.1s | Hit@10=0.0706 NDCG@10=0.0379 | Hit@50=0.1869 NDCG@50=0.0628\n",
      "Best NDCG@10 so far: 0.03794 (epoch 9), patience=0/4\n",
      "Epoch 10 | loss=0.2296 | steps=76 | time=37.1s | Hit@10=0.0721 NDCG@10=0.0388 | Hit@50=0.1925 NDCG@50=0.0646\n",
      "Best NDCG@10 so far: 0.03880 (epoch 10), patience=0/4\n",
      "Epoch 11 | loss=0.2198 | steps=76 | time=35.5s | Hit@10=0.0737 NDCG@10=0.0396 | Hit@50=0.1959 NDCG@50=0.0658\n",
      "Best NDCG@10 so far: 0.03956 (epoch 11), patience=0/4\n",
      "Epoch 12 | loss=0.2122 | steps=76 | time=35.0s | Hit@10=0.0752 NDCG@10=0.0404 | Hit@50=0.2000 NDCG@50=0.0671\n",
      "Best NDCG@10 so far: 0.04038 (epoch 12), patience=0/4\n",
      "Epoch 13 | loss=0.2056 | steps=76 | time=35.4s | Hit@10=0.0764 NDCG@10=0.0410 | Hit@50=0.2041 NDCG@50=0.0684\n",
      "Best NDCG@10 so far: 0.04104 (epoch 13), patience=0/4\n",
      "Epoch 14 | loss=0.1997 | steps=76 | time=36.9s | Hit@10=0.0778 NDCG@10=0.0418 | Hit@50=0.2074 NDCG@50=0.0696\n",
      "Best NDCG@10 so far: 0.04182 (epoch 14), patience=0/4\n",
      "Epoch 15 | loss=0.1934 | steps=76 | time=35.5s | Hit@10=0.0791 NDCG@10=0.0425 | Hit@50=0.2103 NDCG@50=0.0707\n",
      "Best NDCG@10 so far: 0.04250 (epoch 15), patience=0/4\n",
      "Epoch 16 | loss=0.1885 | steps=76 | time=35.8s | Hit@10=0.0802 NDCG@10=0.0431 | Hit@50=0.2144 NDCG@50=0.0719\n",
      "Best NDCG@10 so far: 0.04313 (epoch 16), patience=0/4\n",
      "Epoch 17 | loss=0.1836 | steps=76 | time=36.1s | Hit@10=0.0813 NDCG@10=0.0439 | Hit@50=0.2176 NDCG@50=0.0731\n",
      "Best NDCG@10 so far: 0.04390 (epoch 17), patience=0/4\n",
      "Epoch 18 | loss=0.1793 | steps=76 | time=36.0s | Hit@10=0.0824 NDCG@10=0.0443 | Hit@50=0.2198 NDCG@50=0.0738\n",
      "Best NDCG@10 so far: 0.04430 (epoch 18), patience=0/4\n",
      "Epoch 19 | loss=0.1756 | steps=76 | time=36.4s | Hit@10=0.0836 NDCG@10=0.0450 | Hit@50=0.2221 NDCG@50=0.0747\n",
      "Best NDCG@10 so far: 0.04498 (epoch 19), patience=0/4\n",
      "Epoch 20 | loss=0.1723 | steps=76 | time=38.6s | Hit@10=0.0845 NDCG@10=0.0455 | Hit@50=0.2245 NDCG@50=0.0755\n",
      "Best NDCG@10 so far: 0.04547 (epoch 20), patience=0/4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>Hit@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>Hit@20</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>Hit@50</th>\n",
       "      <th>NDCG@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>35.127548</td>\n",
       "      <td>0.046163</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>0.074628</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.129443</td>\n",
       "      <td>0.044178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.405779</td>\n",
       "      <td>36.561830</td>\n",
       "      <td>0.048785</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.078224</td>\n",
       "      <td>0.034695</td>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.045826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.369434</td>\n",
       "      <td>35.176867</td>\n",
       "      <td>0.054796</td>\n",
       "      <td>0.029682</td>\n",
       "      <td>0.083561</td>\n",
       "      <td>0.036877</td>\n",
       "      <td>0.143414</td>\n",
       "      <td>0.048676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.336820</td>\n",
       "      <td>34.910926</td>\n",
       "      <td>0.058298</td>\n",
       "      <td>0.031146</td>\n",
       "      <td>0.087794</td>\n",
       "      <td>0.038543</td>\n",
       "      <td>0.151485</td>\n",
       "      <td>0.051065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.314924</td>\n",
       "      <td>35.648708</td>\n",
       "      <td>0.059553</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.090415</td>\n",
       "      <td>0.039579</td>\n",
       "      <td>0.156392</td>\n",
       "      <td>0.052573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>35.744465</td>\n",
       "      <td>0.061313</td>\n",
       "      <td>0.032875</td>\n",
       "      <td>0.093281</td>\n",
       "      <td>0.040883</td>\n",
       "      <td>0.162497</td>\n",
       "      <td>0.054505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.277572</td>\n",
       "      <td>35.580034</td>\n",
       "      <td>0.063935</td>\n",
       "      <td>0.034419</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.042961</td>\n",
       "      <td>0.170044</td>\n",
       "      <td>0.057144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.258024</td>\n",
       "      <td>35.498808</td>\n",
       "      <td>0.067943</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.045183</td>\n",
       "      <td>0.179464</td>\n",
       "      <td>0.060258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.241962</td>\n",
       "      <td>36.143474</td>\n",
       "      <td>0.070639</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.107570</td>\n",
       "      <td>0.047196</td>\n",
       "      <td>0.186861</td>\n",
       "      <td>0.062818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.229626</td>\n",
       "      <td>37.095323</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>0.038801</td>\n",
       "      <td>0.111334</td>\n",
       "      <td>0.048664</td>\n",
       "      <td>0.192498</td>\n",
       "      <td>0.064634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>35.492912</td>\n",
       "      <td>0.073711</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.049575</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.065778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.212196</td>\n",
       "      <td>34.978352</td>\n",
       "      <td>0.075209</td>\n",
       "      <td>0.040382</td>\n",
       "      <td>0.115791</td>\n",
       "      <td>0.050554</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>0.067120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>35.424032</td>\n",
       "      <td>0.076445</td>\n",
       "      <td>0.041036</td>\n",
       "      <td>0.117926</td>\n",
       "      <td>0.051436</td>\n",
       "      <td>0.204053</td>\n",
       "      <td>0.068376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.199669</td>\n",
       "      <td>36.922667</td>\n",
       "      <td>0.077812</td>\n",
       "      <td>0.041822</td>\n",
       "      <td>0.119948</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.207367</td>\n",
       "      <td>0.069578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.193367</td>\n",
       "      <td>35.494135</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.042504</td>\n",
       "      <td>0.122121</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.210270</td>\n",
       "      <td>0.070654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.188468</td>\n",
       "      <td>35.780094</td>\n",
       "      <td>0.080209</td>\n",
       "      <td>0.043126</td>\n",
       "      <td>0.124349</td>\n",
       "      <td>0.054181</td>\n",
       "      <td>0.214446</td>\n",
       "      <td>0.071925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>36.052835</td>\n",
       "      <td>0.081333</td>\n",
       "      <td>0.043898</td>\n",
       "      <td>0.125810</td>\n",
       "      <td>0.055050</td>\n",
       "      <td>0.217592</td>\n",
       "      <td>0.073150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.179306</td>\n",
       "      <td>36.009037</td>\n",
       "      <td>0.082381</td>\n",
       "      <td>0.044298</td>\n",
       "      <td>0.127533</td>\n",
       "      <td>0.055613</td>\n",
       "      <td>0.219802</td>\n",
       "      <td>0.073823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.175626</td>\n",
       "      <td>36.380483</td>\n",
       "      <td>0.083636</td>\n",
       "      <td>0.044985</td>\n",
       "      <td>0.128975</td>\n",
       "      <td>0.056358</td>\n",
       "      <td>0.222106</td>\n",
       "      <td>0.074738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.172306</td>\n",
       "      <td>38.635920</td>\n",
       "      <td>0.084498</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>0.129911</td>\n",
       "      <td>0.056873</td>\n",
       "      <td>0.224465</td>\n",
       "      <td>0.075537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss   time_sec    Hit@10   NDCG@10    Hit@20   NDCG@20  \\\n",
       "0       1  0.574000  35.127548  0.046163  0.026194  0.074628  0.033350   \n",
       "1       2  0.405779  36.561830  0.048785  0.027295  0.078224  0.034695   \n",
       "2       3  0.369434  35.176867  0.054796  0.029682  0.083561  0.036877   \n",
       "3       4  0.336820  34.910926  0.058298  0.031146  0.087794  0.038543   \n",
       "4       5  0.314924  35.648708  0.059553  0.031837  0.090415  0.039579   \n",
       "5       6  0.296569  35.744465  0.061313  0.032875  0.093281  0.040883   \n",
       "6       7  0.277572  35.580034  0.063935  0.034419  0.098000  0.042961   \n",
       "7       8  0.258024  35.498808  0.067943  0.036415  0.102906  0.045183   \n",
       "8       9  0.241962  36.143474  0.070639  0.037942  0.107570  0.047196   \n",
       "9      10  0.229626  37.095323  0.072063  0.038801  0.111334  0.048664   \n",
       "10     11  0.219760  35.492912  0.073711  0.039556  0.113600  0.049575   \n",
       "11     12  0.212196  34.978352  0.075209  0.040382  0.115791  0.050554   \n",
       "12     13  0.205556  35.424032  0.076445  0.041036  0.117926  0.051436   \n",
       "13     14  0.199669  36.922667  0.077812  0.041822  0.119948  0.052369   \n",
       "14     15  0.193367  35.494135  0.079085  0.042504  0.122121  0.053275   \n",
       "15     16  0.188468  35.780094  0.080209  0.043126  0.124349  0.054181   \n",
       "16     17  0.183588  36.052835  0.081333  0.043898  0.125810  0.055050   \n",
       "17     18  0.179306  36.009037  0.082381  0.044298  0.127533  0.055613   \n",
       "18     19  0.175626  36.380483  0.083636  0.044985  0.128975  0.056358   \n",
       "19     20  0.172306  38.635920  0.084498  0.045468  0.129911  0.056873   \n",
       "\n",
       "      Hit@50   NDCG@50  \n",
       "0   0.129443  0.044178  \n",
       "1   0.134537  0.045826  \n",
       "2   0.143414  0.048676  \n",
       "3   0.151485  0.051065  \n",
       "4   0.156392  0.052573  \n",
       "5   0.162497  0.054505  \n",
       "6   0.170044  0.057144  \n",
       "7   0.179464  0.060258  \n",
       "8   0.186861  0.062818  \n",
       "9   0.192498  0.064634  \n",
       "10  0.195869  0.065778  \n",
       "11  0.199951  0.067120  \n",
       "12  0.204053  0.068376  \n",
       "13  0.207367  0.069578  \n",
       "14  0.210270  0.070654  \n",
       "15  0.214446  0.071925  \n",
       "16  0.217592  0.073150  \n",
       "17  0.219802  0.073823  \n",
       "18  0.222106  0.074738  \n",
       "19  0.224465  0.075537  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 15: Optimized LightGCN training with gradient-accumulation blocks\n",
    "# What we do:\n",
    "# - Train LightGCN with BPR loss\n",
    "# - Negative sampling ускоряем: сразу K кандидатов, выбираем первый не-позитив\n",
    "# - Самое главное ускорение: propagate(A_norm) делаем 1 раз на блок ACC_STEPS батчей,\n",
    "#   а не на каждый батч (иначе будет адски медленно)\n",
    "# - На одном graph делаем несколько backward(): retain_graph=True для всех, кроме последнего\n",
    "# - После каждой эпохи считаем ranking metrics на validation (Hit/NDCG@K)\n",
    "\n",
    "# ---------------------------\n",
    "# Fast negative sampling\n",
    "# ---------------------------\n",
    "def sample_negatives_fast(users_np: np.ndarray, train_pos: dict[int, set], B: int, K_try: int = 20) -> np.ndarray:\n",
    "    users_np = users_np.astype(np.int32)\n",
    "    out = np.empty(len(users_np), dtype=np.int32)\n",
    "    cand = np.random.randint(0, B, size=(len(users_np), K_try), dtype=np.int32)\n",
    "\n",
    "    for r, u in enumerate(users_np):\n",
    "        seen = train_pos[int(u)]\n",
    "        chosen = None\n",
    "        row = cand[r]\n",
    "        for neg in row:\n",
    "            neg = int(neg)\n",
    "            if neg not in seen:\n",
    "                chosen = neg\n",
    "                break\n",
    "        if chosen is None:\n",
    "            while True:\n",
    "                neg = int(np.random.randint(0, B))\n",
    "                if neg not in seen:\n",
    "                    chosen = neg\n",
    "                    break\n",
    "        out[r] = chosen\n",
    "    return out\n",
    "\n",
    "def bpr_loss(u_emb, pos_emb, neg_emb):\n",
    "    pos_scores = (u_emb * pos_emb).sum(dim=1)\n",
    "    neg_scores = (u_emb * neg_emb).sum(dim=1)\n",
    "    return -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8).mean()\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparams (safe + fast)\n",
    "# ---------------------------\n",
    "EMB_DIM = 64\n",
    "NUM_LAYERS = 3\n",
    "LR = 2e-3            \n",
    "WEIGHT_DECAY = 0.0  \n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "ACC_STEPS = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "USER_BATCH_EVAL = 1024\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 4\n",
    "MIN_DELTA = 1e-5\n",
    "\n",
    "print(\"Training config:\",\n",
    "      {\"EMB_DIM\": EMB_DIM, \"NUM_LAYERS\": NUM_LAYERS, \"LR\": LR,\n",
    "       \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "       \"BATCH_SIZE\": BATCH_SIZE, \"ACC_STEPS\": ACC_STEPS, \"EPOCHS\": EPOCHS,\n",
    "       \"USER_BATCH_EVAL\": USER_BATCH_EVAL,\n",
    "       \"PATIENCE\": PATIENCE, \"MIN_DELTA\": MIN_DELTA})\n",
    "\n",
    "# ---------------------------\n",
    "# Model + optimizer\n",
    "# ---------------------------\n",
    "model = LightGCN(num_nodes=num_nodes, emb_dim=EMB_DIM, num_layers=NUM_LAYERS, dropout=0.0).to(DEVICE)\n",
    "opt = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Train arrays (idx-space)\n",
    "train_users = train_df[\"user_idx\"].to_numpy(dtype=np.int32)\n",
    "train_items = train_df[\"book_idx\"].to_numpy(dtype=np.int32)\n",
    "num_inter = len(train_users)\n",
    "perm = np.arange(num_inter)\n",
    "\n",
    "history = []\n",
    "best_ndcg10 = -1.0\n",
    "best_epoch = 0\n",
    "pat_cnt = 0\n",
    "\n",
    "# ---------------------------\n",
    "# Training\n",
    "# ---------------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    np.random.shuffle(perm)\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    block_size = BATCH_SIZE * ACC_STEPS\n",
    "\n",
    "    for block_start in range(0, num_inter, block_size):\n",
    "        # 1) propagate once per block (graph attached)\n",
    "        all_emb = model.propagate(A_norm)  # [N, D]\n",
    "\n",
    "        # 2) build a single loss tensor for the whole block\n",
    "        loss_block = 0.0\n",
    "\n",
    "        # figure out how many inner batches\n",
    "        remaining = num_inter - block_start\n",
    "        n_inner = min(ACC_STEPS, int(np.ceil(remaining / BATCH_SIZE)))\n",
    "\n",
    "        for inner in range(n_inner):\n",
    "            start = block_start + inner * BATCH_SIZE\n",
    "            batch_idx = perm[start:start + BATCH_SIZE]\n",
    "            bu = train_users[batch_idx]\n",
    "            bi = train_items[batch_idx]\n",
    "            bn = sample_negatives_fast(bu, train_pos, B, K_try=20)\n",
    "\n",
    "            u_nodes = torch.from_numpy(bu.astype(np.int64) + user_offset).to(DEVICE)\n",
    "            p_nodes = torch.from_numpy(bi.astype(np.int64) + book_offset).to(DEVICE)\n",
    "            n_nodes = torch.from_numpy(bn.astype(np.int64) + book_offset).to(DEVICE)\n",
    "\n",
    "            u_emb = all_emb[u_nodes]\n",
    "            p_emb = all_emb[p_nodes]\n",
    "            n_emb = all_emb[n_nodes]\n",
    "\n",
    "            loss_block = loss_block + bpr_loss(u_emb, p_emb, n_emb)\n",
    "            n_batches += 1\n",
    "\n",
    "        loss_block = loss_block / n_inner\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss_block.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += float(loss_block.item())\n",
    "        n_steps += 1\n",
    "\n",
    "        del all_emb, loss_block\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_emb_eval = model.propagate(A_norm)\n",
    "\n",
    "    val_metrics = eval_ranking(\n",
    "        all_emb_eval,\n",
    "        val_gt,\n",
    "        train_pos,\n",
    "        K_list=(10, 20, 50),\n",
    "        user_batch_size=USER_BATCH_EVAL,\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    row = {\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": epoch_loss / max(n_steps, 1),\n",
    "        \"time_sec\": dt,\n",
    "        **val_metrics\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | loss={row['loss']:.4f} | steps={n_steps} | time={dt:.1f}s | \"\n",
    "        f\"Hit@10={row['Hit@10']:.4f} NDCG@10={row['NDCG@10']:.4f} | \"\n",
    "        f\"Hit@50={row['Hit@50']:.4f} NDCG@50={row['NDCG@50']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ---- Early stopping on NDCG@10 ----\n",
    "    cur = row[\"NDCG@10\"]\n",
    "    if cur > best_ndcg10 + MIN_DELTA:\n",
    "        best_ndcg10 = cur\n",
    "        best_epoch = epoch\n",
    "        pat_cnt = 0\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "\n",
    "    print(f\"Best NDCG@10 so far: {best_ndcg10:.5f} (epoch {best_epoch}), patience={pat_cnt}/{PATIENCE}\")\n",
    "\n",
    "    if pat_cnt >= PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6bd2168-38ff-47eb-ba66-155e47a88816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST metrics (augmented graph):\n",
      "  Hit@10: 0.082775\n",
      "  NDCG@10: 0.044579\n",
      "  Hit@20: 0.127439\n",
      "  NDCG@20: 0.055769\n",
      "  Hit@50: 0.220982\n",
      "  NDCG@50: 0.074206\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Final evaluation on TEST (leave-one-out)\n",
    "# - same ranking metrics as on val\n",
    "# - ground truth: test_gt (1 item per user)\n",
    "# - filter: train_pos (do NOT recommend items seen in train)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_emb_test = model.propagate(A_norm)\n",
    "\n",
    "test_metrics = eval_ranking(\n",
    "    all_emb_test,\n",
    "    test_gt,\n",
    "    train_pos,\n",
    "    K_list=(10, 20, 50),\n",
    "    user_batch_size=USER_BATCH_EVAL,\n",
    ")\n",
    "\n",
    "print(\"TEST metrics (augmented graph):\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdfae7f3-98c4-43b4-88c1-740fa2661284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>baseline</th>\n",
       "      <th>augmented_test</th>\n",
       "      <th>delta_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hit@10</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.082775</td>\n",
       "      <td>-0.001225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDCG@10</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044579</td>\n",
       "      <td>-0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hit@20</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.127439</td>\n",
       "      <td>-0.001561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDCG@20</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.055769</td>\n",
       "      <td>-0.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hit@50</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.220982</td>\n",
       "      <td>-0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NDCG@50</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>-0.000794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    metric  baseline  augmented_test  delta_abs\n",
       "0   Hit@10     0.084        0.082775  -0.001225\n",
       "1  NDCG@10     0.045        0.044579  -0.000421\n",
       "2   Hit@20     0.129        0.127439  -0.001561\n",
       "3  NDCG@20     0.057        0.055769  -0.001231\n",
       "4   Hit@50     0.221        0.220982  -0.000018\n",
       "5  NDCG@50     0.075        0.074206  -0.000794"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 17: Compare augmented TEST metrics vs baseline reference\n",
    "# - baseline_ref: numbers from your baseline (user–book LightGCN + hard negatives)\n",
    "# - show absolute deltas\n",
    "\n",
    "baseline_ref = {\n",
    "    \"Hit@10\": 0.084,\n",
    "    \"NDCG@10\": 0.045,\n",
    "    \"Hit@20\": 0.129,\n",
    "    \"NDCG@20\": 0.057,\n",
    "    \"Hit@50\": 0.221,\n",
    "    \"NDCG@50\": 0.075,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for m in [\"Hit@10\",\"NDCG@10\",\"Hit@20\",\"NDCG@20\",\"Hit@50\",\"NDCG@50\"]:\n",
    "    aug = float(test_metrics.get(m, np.nan))\n",
    "    base = float(baseline_ref.get(m, np.nan))\n",
    "    rows.append({\n",
    "        \"metric\": m,\n",
    "        \"baseline\": base,\n",
    "        \"augmented_test\": aug,\n",
    "        \"delta_abs\": aug - base\n",
    "    })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows)\n",
    "cmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75c0da38-e3e1-49ea-b65a-90d9d218d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books with >=1 tag edge: 812 / 9999  (8.12%)\n",
      "Tags per tagged-book: mean= 19.998768472906406 median= 20.0 p90= 20.0 max= 20\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Diagnose tag coverage (why augmentation may not help)\n",
    "# - how many books have >=1 tag edge?\n",
    "# - what fraction of catalog is covered?\n",
    "# - distribution of tags per book (after filtering)\n",
    "\n",
    "books_with_tags = filtered_bt[\"book_idx\"].nunique()\n",
    "coverage = books_with_tags / B\n",
    "\n",
    "tags_per_book = filtered_bt.groupby(\"book_idx\")[\"tag_idx\"].nunique()\n",
    "\n",
    "print(f\"Books with >=1 tag edge: {books_with_tags} / {B}  ({coverage*100:.2f}%)\")\n",
    "print(\"Tags per tagged-book: mean=\", float(tags_per_book.mean()),\n",
    "      \"median=\", float(tags_per_book.median()),\n",
    "      \"p90=\", float(tags_per_book.quantile(0.9)),\n",
    "      \"max=\", int(tags_per_book.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669086db-cd55-4deb-b0a5-f29a5436b0c7",
   "metadata": {},
   "source": [
    "## Stage Summary: Graph Augmentation v1 (user–book–tag)\n",
    "\n",
    "### Goal of this stage\n",
    "To test whether **augmenting a pure user–item graph with content-based relations**\n",
    "(book–tag edges) can **break the performance ceiling of collaborative filtering**\n",
    "on the **GoodBooks-10k** dataset.\n",
    "\n",
    "## What was done\n",
    "\n",
    "### Baseline graph (reference)\n",
    "- A **bipartite user–book graph** trained with **LightGCN + BPR loss**\n",
    "- Proper **leave-one-out split per user**:\n",
    "  - train: all interactions except the last ones\n",
    "  - validation: 1 item per user\n",
    "  - test: 1 item per user\n",
    "- Evaluation with **ranking metrics only** (Hit@K, NDCG@K)\n",
    "- Previously established CF ceiling:\n",
    "  - **NDCG@10 ≈ 0.045**\n",
    "  - **Hit@10 ≈ 0.084**\n",
    "\n",
    "This baseline serves as a **fixed reference point** for all subsequent experiments.\n",
    "\n",
    "### Graph Augmentation v1\n",
    "- Introduced **content nodes (tags)** into the graph\n",
    "- Constructed an augmented topology:\n",
    "\n",
    "user — book — tag — book\n",
    "\n",
    "\n",
    "- Data sources:\n",
    "- `tags.csv`\n",
    "- `book_tags.csv`\n",
    "- Tag filtering strategy:\n",
    "- `MIN_BOOK_FREQ = 50`\n",
    "- `TOP_TAGS_PER_BOOK = 20`\n",
    "- Final graph statistics:\n",
    "- users: 53,398\n",
    "- books: 9,999\n",
    "- tags: 265\n",
    "- total nodes: ~63k\n",
    "\n",
    "- Training setup:\n",
    "- LightGCN on the augmented graph\n",
    "- same BPR loss\n",
    "- **train-only edges** (no validation/test leakage)\n",
    "\n",
    "## Results (TEST set)\n",
    "\n",
    "**Augmented graph (v1):**\n",
    "- Hit@10 = **0.0828**\n",
    "- NDCG@10 = **0.0446**\n",
    "- Hit@50 = **0.2210**\n",
    "- NDCG@50 = **0.0742**\n",
    "\n",
    "**Comparison with baseline:**\n",
    "\n",
    "| Metric   | Baseline | Augmented v1 | Δ |\n",
    "|----------|----------|--------------|---|\n",
    "| Hit@10   | 0.0840   | 0.0828       | −0.0012 |\n",
    "| NDCG@10  | 0.0450   | 0.0446       | −0.0004 |\n",
    "| Hit@50   | 0.2210   | 0.2210       | ≈ 0 |\n",
    "| NDCG@50  | 0.0750   | 0.0742       | −0.0008 |\n",
    "\n",
    "The augmented model performs **on par with the baseline**, with minor differences\n",
    "within statistical noise.\n",
    "\n",
    "## Why augmentation v1 did not improve performance\n",
    "\n",
    "### A key diagnostic finding:\n",
    "\n",
    "- Only **812 out of 9,999 books** are connected to at least one tag\n",
    "- Tag coverage of the catalog: **≈ 8.1%**\n",
    "\n",
    "```text\n",
    "Books with ≥1 tag edge: 812 / 9999 (8.12%)\n",
    "Tags per tagged book: mean ≈ 20\n",
    "\n",
    "## As a result:\n",
    "\n",
    "For over 90% of items, the graph remains purely user–book\n",
    "\n",
    "LightGCN relies almost entirely on collaborative signals\n",
    "\n",
    "The content signal is too sparse to affect global ranking\n",
    "\n",
    "This empirically confirms the baseline conclusion:\n",
    "\n",
    "Architectural complexity alone does not break the CF ceiling\n",
    "unless the graph is enriched with sufficiently dense semantic relations.\n",
    "\n",
    "## Key takeaway\n",
    "\n",
    "Graph augmentation as a concept is valid, but:\n",
    "\n",
    "❌ the current tag configuration is too sparse\n",
    "\n",
    "❌ content edges affect only a small fraction of the catalog\n",
    "\n",
    "✔ the pipeline is correct and leakage-free\n",
    "\n",
    "✔ the CF ceiling is faithfully reproduced\n",
    "\n",
    "This stage validates the engineering setup, but is not the final solution.\n",
    "\n",
    "Next steps: Graph Augmentation v2\n",
    "\n",
    "The next experiment focuses on strengthening the graph itself, not the model.\n",
    "\n",
    "## Plan for v2:\n",
    "\n",
    " - Increase tag coverage\n",
    "\n",
    "reduce MIN_BOOK_FREQ (e.g., 50 → 10)\n",
    "\n",
    "increase TOP_TAGS_PER_BOOK (20 → 50)\n",
    "\n",
    "target: ≥ 50% of books connected to tags\n",
    "\n",
    "- (Optional) Weighted book–tag edges\n",
    "\n",
    "use the count field from book_tags.csv\n",
    "\n",
    "edge weights such as log1p(count) or sqrt(count)\n",
    "\n",
    "- Re-train the same LightGCN\n",
    "\n",
    "identical splits\n",
    "\n",
    "identical metrics\n",
    "\n",
    "clean comparison with baseline and v1\n",
    "\n",
    "Only after this step does it make sense to explore:\n",
    "\n",
    "heterogeneous GNNs (GraphSAGE, R-GCN)\n",
    "\n",
    "or hybrid content-aware recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c0b66a3-fcfe-4f3d-962d-50c3afbccf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_tags_raw: (999912, 3)\n",
      "unique goodreads_book_id in raw: 10000\n",
      "missing g2b mappings: 0\n",
      "book_tags_raw after mapping: (999912, 4)\n",
      "unique book_id after mapping: 10000\n"
     ]
    }
   ],
   "source": [
    "# Cell 5c: Reload raw book_tags.csv WITHOUT any filtering and map goodreads_book_id -> book_id\n",
    "# Why:\n",
    "# - book_tags.csv uses goodreads_book_id\n",
    "# - our interactions/mapping use book_id\n",
    "# - previous filtering was wrong (goodreads_book_id vs book_id), causing only 812 books\n",
    "\n",
    "books_path = DATA_RAW / \"books.csv\"\n",
    "book_tags_path = DATA_RAW / RAW_FILES[\"book_tags\"]\n",
    "tags_path = DATA_RAW / RAW_FILES[\"tags\"]\n",
    "\n",
    "books_df = pd.read_csv(books_path)\n",
    "tags_df = pd.read_csv(tags_path)\n",
    "\n",
    "assert {\"book_id\", \"goodreads_book_id\"}.issubset(books_df.columns), \\\n",
    "    \"books.csv must contain book_id and goodreads_book_id\"\n",
    "assert {\"tag_id\", \"tag_name\"}.issubset(tags_df.columns), \\\n",
    "    \"tags.csv must contain tag_id and tag_name\"\n",
    "\n",
    "# load RAW (no filtering!)\n",
    "book_tags_raw = pd.read_csv(book_tags_path)\n",
    "\n",
    "# schema checks\n",
    "assert {\"goodreads_book_id\", \"tag_id\", \"count\"}.issubset(book_tags_raw.columns), \\\n",
    "    \"book_tags.csv must contain goodreads_book_id, tag_id, count\"\n",
    "\n",
    "book_tags_raw[\"goodreads_book_id\"] = book_tags_raw[\"goodreads_book_id\"].astype(int)\n",
    "book_tags_raw[\"tag_id\"] = book_tags_raw[\"tag_id\"].astype(int)\n",
    "book_tags_raw[\"count\"] = book_tags_raw[\"count\"].astype(int)\n",
    "\n",
    "print(\"book_tags_raw:\", book_tags_raw.shape)\n",
    "print(\"unique goodreads_book_id in raw:\", book_tags_raw[\"goodreads_book_id\"].nunique())\n",
    "\n",
    "# mapping: goodreads_book_id -> book_id\n",
    "g2b = dict(zip(books_df[\"goodreads_book_id\"].astype(int), books_df[\"book_id\"].astype(int)))\n",
    "\n",
    "book_tags_raw[\"book_id\"] = book_tags_raw[\"goodreads_book_id\"].map(g2b)\n",
    "missing = book_tags_raw[\"book_id\"].isna().sum()\n",
    "print(\"missing g2b mappings:\", missing)\n",
    "\n",
    "book_tags_raw = book_tags_raw.dropna(subset=[\"book_id\"]).copy()\n",
    "book_tags_raw[\"book_id\"] = book_tags_raw[\"book_id\"].astype(int)\n",
    "\n",
    "print(\"book_tags_raw after mapping:\", book_tags_raw.shape)\n",
    "print(\"unique book_id after mapping:\", book_tags_raw[\"book_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bf419d5-5241-42ef-881d-a69282314e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_tags_df_fixed: (999856, 4)\n",
      "unique books with tags (fixed): 9999\n",
      "unique tags in relations (fixed): 34225\n"
     ]
    }
   ],
   "source": [
    "# Cell 5d: Correct filtering to our model catalog (book_id space)\n",
    "# - keep only rows where mapped book_id is present in our book2idx keys (book_id)\n",
    "# Result: book_tags_df_fixed should cover ~all books (near 9999)\n",
    "\n",
    "book_ids_in_mapping = set(book2idx.keys())\n",
    "\n",
    "book_tags_df_fixed = book_tags_raw[book_tags_raw[\"book_id\"].isin(book_ids_in_mapping)].copy()\n",
    "\n",
    "print(\"book_tags_df_fixed:\", book_tags_df_fixed.shape)\n",
    "print(\"unique books with tags (fixed):\", book_tags_df_fixed[\"book_id\"].nunique())\n",
    "print(\"unique tags in relations (fixed):\", book_tags_df_fixed[\"tag_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37c06edb-649b-4dc9-b1c2-4f5b2b0061e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After MIN_BOOK_FREQ: (938635, 4) | tags: 5503\n",
      "After TOP_TAGS_PER_BOOK: (499860, 4) | tags: 5014\n",
      "Final tag vocab size T2: 5014\n",
      "Books with >=1 tag edge: 9999 / 9999 (100.00%)\n",
      "Tags per tagged-book: mean= 49.99069906990699 median= 50.0 p90= 50.0 max= 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619294</th>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>50755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619295</th>\n",
       "      <td>0</td>\n",
       "      <td>1271</td>\n",
       "      <td>35418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619296</th>\n",
       "      <td>0</td>\n",
       "      <td>4937</td>\n",
       "      <td>25968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619297</th>\n",
       "      <td>0</td>\n",
       "      <td>1725</td>\n",
       "      <td>13819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619298</th>\n",
       "      <td>0</td>\n",
       "      <td>1455</td>\n",
       "      <td>12985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_idx  tag_idx  count\n",
       "619294         0     1688  50755\n",
       "619295         0     1271  35418\n",
       "619296         0     4937  25968\n",
       "619297         0     1725  13819\n",
       "619298         0     1455  12985"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 19 (FIXED v2): Re-filter book_tags using CORRECT book_id space (full coverage)\n",
    "# What we do:\n",
    "# 1) Use book_tags_df_fixed (already mapped goodreads_book_id -> book_id and filtered to our catalog)\n",
    "# 2) Keep tags that appear in >= MIN_BOOK_FREQ books\n",
    "# 3) For each book, keep TOP_TAGS_PER_BOOK strongest tags by 'count'\n",
    "# 4) Build local tag vocabulary (tag2idx_local2) and a compact edge table filtered_bt2\n",
    "#\n",
    "# Output:\n",
    "# - filtered_bt2 with columns: [book_idx, tag_idx, count]\n",
    "# - tag2idx_local2 dict + stats (coverage should be ~9999 books)\n",
    "\n",
    "MIN_BOOK_FREQ = 10        # tag must appear in >= this many books (tunable)\n",
    "TOP_K_TAGS = None         # optional: keep only top-K tags globally by freq (start with None)\n",
    "TOP_TAGS_PER_BOOK = 50    # keep top tags per book by count\n",
    "\n",
    "assert \"book_id\" in book_tags_df_fixed.columns, \"book_tags_df_fixed must contain book_id\"\n",
    "assert \"tag_id\" in book_tags_df_fixed.columns and \"count\" in book_tags_df_fixed.columns, \\\n",
    "    \"book_tags_df_fixed must contain tag_id and count\"\n",
    "\n",
    "# ---- tag frequency by number of distinct books ----\n",
    "tag_book_freq = (\n",
    "    book_tags_df_fixed.groupby(\"tag_id\")[\"book_id\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "eligible_tags = tag_book_freq[tag_book_freq >= MIN_BOOK_FREQ].index\n",
    "filtered2 = book_tags_df_fixed[book_tags_df_fixed[\"tag_id\"].isin(eligible_tags)].copy()\n",
    "print(\"After MIN_BOOK_FREQ:\", filtered2.shape, \"| tags:\", filtered2[\"tag_id\"].nunique())\n",
    "\n",
    "if TOP_K_TAGS is not None:\n",
    "    top_tags = tag_book_freq.loc[eligible_tags].head(TOP_K_TAGS).index\n",
    "    filtered2 = filtered2[filtered2[\"tag_id\"].isin(top_tags)].copy()\n",
    "    print(\"After TOP_K_TAGS:\", filtered2.shape, \"| tags:\", filtered2[\"tag_id\"].nunique())\n",
    "\n",
    "# ---- per-book top tags by count ----\n",
    "filtered2 = (\n",
    "    filtered2.sort_values([\"book_id\", \"count\"], ascending=[True, False])\n",
    "             .groupby(\"book_id\", as_index=False)\n",
    "             .head(TOP_TAGS_PER_BOOK)\n",
    "             .copy()\n",
    ")\n",
    "\n",
    "print(\"After TOP_TAGS_PER_BOOK:\", filtered2.shape, \"| tags:\", filtered2[\"tag_id\"].nunique())\n",
    "\n",
    "# ---- map to idx space (book_id -> book_idx) ----\n",
    "filtered2[\"book_idx\"] = filtered2[\"book_id\"].map(book2idx)\n",
    "\n",
    "# Safety: drop any unmapped (shouldn't happen, but keep robust)\n",
    "filtered2 = filtered2.dropna(subset=[\"book_idx\"]).copy()\n",
    "filtered2[\"book_idx\"] = filtered2[\"book_idx\"].astype(np.int64)\n",
    "\n",
    "# ---- build local tag vocabulary ----\n",
    "unique_tag_ids2 = np.sort(filtered2[\"tag_id\"].unique())\n",
    "tag2idx_local2 = {int(tid): i for i, tid in enumerate(unique_tag_ids2)}\n",
    "filtered2[\"tag_idx\"] = filtered2[\"tag_id\"].map(tag2idx_local2).astype(np.int64)\n",
    "\n",
    "# final compact edges table\n",
    "filtered_bt2 = filtered2[[\"book_idx\", \"tag_idx\", \"count\"]].copy()\n",
    "\n",
    "# ---- diagnostics ----\n",
    "books_with_tags2 = int(filtered_bt2[\"book_idx\"].nunique())\n",
    "coverage2 = books_with_tags2 / num_items\n",
    "\n",
    "tags_per_book2 = filtered_bt2.groupby(\"book_idx\")[\"tag_idx\"].nunique()\n",
    "print(\"Final tag vocab size T2:\", len(tag2idx_local2))\n",
    "print(f\"Books with >=1 tag edge: {books_with_tags2} / {num_items} ({coverage2*100:.2f}%)\")\n",
    "print(\"Tags per tagged-book: mean=\", float(tags_per_book2.mean()),\n",
    "      \"median=\", float(tags_per_book2.median()),\n",
    "      \"p90=\", float(tags_per_book2.quantile(0.9)),\n",
    "      \"max=\", int(tags_per_book2.max()))\n",
    "\n",
    "filtered_bt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7e1f2cb-8072-4af8-8b75-bfa21a0c03d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=53398 | B=9999 | T2=5014 | num_nodes2=68411\n",
      "Offsets: {'user': 0, 'book': np.int64(53398), 'tag': np.int64(63397)}\n",
      "edge_index2: torch.Size([2, 10852488]) | directed edges: 10852488\n",
      "A_norm2: (68411, 68411) nnz: 10852482\n",
      "deg stats: 1.0 158.6365966796875 19502.0\n",
      "Graph2 build sanity ✔\n"
     ]
    }
   ],
   "source": [
    "# Cell 20 (FIXED): Build augmented graph v2 (full tag coverage)\n",
    "# - Nodes: users [0..U), books [U..U+B), tags [U+B..U+B+T2)\n",
    "# - Edges:\n",
    "#   * user<->book from TRAIN only (no leakage)\n",
    "#   * book<->tag from filtered_bt2\n",
    "# - Output: A_norm2 (normalized sparse adjacency on DEVICE), plus offsets and sizes\n",
    "\n",
    "U = num_users\n",
    "B = num_items\n",
    "T2 = len(tag2idx_local2)\n",
    "\n",
    "user_offset2 = 0\n",
    "book_offset2 = U\n",
    "tag_offset2  = U + B\n",
    "num_nodes2   = U + B + T2\n",
    "\n",
    "print(f\"U={U} | B={B} | T2={T2} | num_nodes2={num_nodes2}\")\n",
    "print(\"Offsets:\", {\"user\": user_offset2, \"book\": book_offset2, \"tag\": tag_offset2})\n",
    "\n",
    "# ---- user-book edges from TRAIN (idx-space) ----\n",
    "train_u = train_df[\"user_idx\"].to_numpy(dtype=np.int64)\n",
    "train_b = train_df[\"book_idx\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "src_ub = torch.from_numpy(train_u + user_offset2).long()\n",
    "dst_ub = torch.from_numpy(train_b + book_offset2).long()\n",
    "\n",
    "edge_src = torch.cat([src_ub, dst_ub], dim=0)\n",
    "edge_dst = torch.cat([dst_ub, src_ub], dim=0)\n",
    "\n",
    "# ---- book-tag edges from filtered_bt2 ----\n",
    "bt_books = filtered_bt2[\"book_idx\"].to_numpy(dtype=np.int64)\n",
    "bt_tags  = filtered_bt2[\"tag_idx\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "src_bt = torch.from_numpy(bt_books + book_offset2).long()\n",
    "dst_bt = torch.from_numpy(bt_tags  + tag_offset2).long()\n",
    "\n",
    "edge_src = torch.cat([edge_src, src_bt, dst_bt], dim=0)\n",
    "edge_dst = torch.cat([edge_dst, dst_bt, src_bt], dim=0)\n",
    "\n",
    "edge_index2 = torch.stack([edge_src, edge_dst], dim=0)\n",
    "E2 = edge_index2.size(1)\n",
    "\n",
    "print(\"edge_index2:\", edge_index2.shape, \"| directed edges:\", E2)\n",
    "\n",
    "# ---- build normalized adjacency ----\n",
    "val = torch.ones(E2, dtype=torch.float32)\n",
    "A2 = torch.sparse_coo_tensor(edge_index2, val, (num_nodes2, num_nodes2)).coalesce()\n",
    "\n",
    "deg2 = torch.sparse.sum(A2, dim=1).to_dense()\n",
    "deg_inv_sqrt2 = torch.pow(deg2.clamp(min=1.0), -0.5)\n",
    "\n",
    "row, col = A2.indices()\n",
    "norm_val = deg_inv_sqrt2[row] * A2.values() * deg_inv_sqrt2[col]\n",
    "\n",
    "A_norm2 = torch.sparse_coo_tensor(A2.indices(), norm_val, A2.size()).coalesce().to(DEVICE)\n",
    "\n",
    "print(\"A_norm2:\", tuple(A_norm2.shape), \"nnz:\", A_norm2._nnz())\n",
    "print(\"deg stats:\", float(deg2.min()), float(deg2.mean()), float(deg2.max()))\n",
    "print(\"Graph2 build sanity ✔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0102e35d-8cf8-4c9b-8cf4-4cfcb1132871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config: {'EMB_DIM': 64, 'NUM_LAYERS': 3, 'LR': 0.002, 'BATCH_SIZE': 4096, 'ACC_STEPS': 16, 'EPOCHS': 30, 'USER_BATCH_EVAL': 1024, 'PATIENCE': 5, 'MIN_DELTA': 0.0001}\n",
      "[Graph2] Epoch 01 | loss=0.5785 | time=36.9s | Hit@10=0.0461 NDCG@10=0.0260 | Hit@50=0.1293 NDCG@50=0.0440\n",
      "  ✅ New best NDCG@10=0.02604 at epoch 1\n",
      "[Graph2] Epoch 02 | loss=0.4066 | time=37.5s | Hit@10=0.0485 NDCG@10=0.0272 | Hit@50=0.1337 NDCG@50=0.0456\n",
      "  ✅ New best NDCG@10=0.02721 at epoch 2\n",
      "[Graph2] Epoch 03 | loss=0.3710 | time=37.1s | Hit@10=0.0541 NDCG@10=0.0297 | Hit@50=0.1420 NDCG@50=0.0485\n",
      "  ✅ New best NDCG@10=0.02967 at epoch 3\n",
      "[Graph2] Epoch 04 | loss=0.3380 | time=37.8s | Hit@10=0.0574 NDCG@10=0.0310 | Hit@50=0.1493 NDCG@50=0.0507\n",
      "  ✅ New best NDCG@10=0.03099 at epoch 4\n",
      "[Graph2] Epoch 05 | loss=0.3150 | time=37.6s | Hit@10=0.0584 NDCG@10=0.0317 | Hit@50=0.1548 NDCG@50=0.0524\n",
      "  ✅ New best NDCG@10=0.03174 at epoch 5\n",
      "[Graph2] Epoch 06 | loss=0.2962 | time=37.5s | Hit@10=0.0608 NDCG@10=0.0328 | Hit@50=0.1602 NDCG@50=0.0541\n",
      "  ✅ New best NDCG@10=0.03282 at epoch 6\n",
      "[Graph2] Epoch 07 | loss=0.2751 | time=36.9s | Hit@10=0.0639 NDCG@10=0.0345 | Hit@50=0.1706 NDCG@50=0.0574\n",
      "  ✅ New best NDCG@10=0.03453 at epoch 7\n",
      "[Graph2] Epoch 08 | loss=0.2535 | time=37.3s | Hit@10=0.0678 NDCG@10=0.0366 | Hit@50=0.1809 NDCG@50=0.0608\n",
      "  ✅ New best NDCG@10=0.03660 at epoch 8\n",
      "[Graph2] Epoch 09 | loss=0.2363 | time=39.0s | Hit@10=0.0709 NDCG@10=0.0381 | Hit@50=0.1878 NDCG@50=0.0632\n",
      "  ✅ New best NDCG@10=0.03814 at epoch 9\n",
      "[Graph2] Epoch 10 | loss=0.2235 | time=37.7s | Hit@10=0.0727 NDCG@10=0.0392 | Hit@50=0.1934 NDCG@50=0.0650\n",
      "  ✅ New best NDCG@10=0.03916 at epoch 10\n",
      "[Graph2] Epoch 11 | loss=0.2142 | time=37.7s | Hit@10=0.0742 NDCG@10=0.0399 | Hit@50=0.1971 NDCG@50=0.0663\n",
      "  ✅ New best NDCG@10=0.03994 at epoch 11\n",
      "[Graph2] Epoch 12 | loss=0.2067 | time=38.3s | Hit@10=0.0759 NDCG@10=0.0407 | Hit@50=0.2003 NDCG@50=0.0674\n",
      "  ✅ New best NDCG@10=0.04073 at epoch 12\n",
      "[Graph2] Epoch 13 | loss=0.2002 | time=37.5s | Hit@10=0.0769 NDCG@10=0.0413 | Hit@50=0.2036 NDCG@50=0.0685\n",
      "  ✅ New best NDCG@10=0.04130 at epoch 13\n",
      "[Graph2] Epoch 14 | loss=0.1945 | time=37.7s | Hit@10=0.0779 NDCG@10=0.0419 | Hit@50=0.2064 NDCG@50=0.0694\n",
      "  ✅ New best NDCG@10=0.04187 at epoch 14\n",
      "[Graph2] Epoch 15 | loss=0.1893 | time=38.5s | Hit@10=0.0792 NDCG@10=0.0425 | Hit@50=0.2102 NDCG@50=0.0706\n",
      "  ✅ New best NDCG@10=0.04249 at epoch 15\n",
      "[Graph2] Epoch 16 | loss=0.1843 | time=37.9s | Hit@10=0.0803 NDCG@10=0.0432 | Hit@50=0.2142 NDCG@50=0.0719\n",
      "  ✅ New best NDCG@10=0.04322 at epoch 16\n",
      "[Graph2] Epoch 17 | loss=0.1802 | time=37.1s | Hit@10=0.0812 NDCG@10=0.0438 | Hit@50=0.2172 NDCG@50=0.0730\n",
      "  ✅ New best NDCG@10=0.04382 at epoch 17\n",
      "[Graph2] Epoch 18 | loss=0.1763 | time=37.1s | Hit@10=0.0825 NDCG@10=0.0446 | Hit@50=0.2200 NDCG@50=0.0740\n",
      "  ✅ New best NDCG@10=0.04457 at epoch 18\n",
      "[Graph2] Epoch 19 | loss=0.1729 | time=37.9s | Hit@10=0.0836 NDCG@10=0.0450 | Hit@50=0.2219 NDCG@50=0.0747\n",
      "  ✅ New best NDCG@10=0.04503 at epoch 19\n",
      "[Graph2] Epoch 20 | loss=0.1690 | time=37.1s | Hit@10=0.0843 NDCG@10=0.0455 | Hit@50=0.2241 NDCG@50=0.0756\n",
      "  ✅ New best NDCG@10=0.04552 at epoch 20\n",
      "[Graph2] Epoch 21 | loss=0.1665 | time=36.6s | Hit@10=0.0853 NDCG@10=0.0460 | Hit@50=0.2256 NDCG@50=0.0762\n",
      "  ✅ New best NDCG@10=0.04599 at epoch 21\n",
      "[Graph2] Epoch 22 | loss=0.1640 | time=37.8s | Hit@10=0.0859 NDCG@10=0.0464 | Hit@50=0.2279 NDCG@50=0.0770\n",
      "  ✅ New best NDCG@10=0.04643 at epoch 22\n",
      "[Graph2] Epoch 23 | loss=0.1617 | time=38.4s | Hit@10=0.0864 NDCG@10=0.0467 | Hit@50=0.2286 NDCG@50=0.0773\n",
      "  ✅ New best NDCG@10=0.04670 at epoch 23\n",
      "[Graph2] Epoch 24 | loss=0.1595 | time=38.1s | Hit@10=0.0867 NDCG@10=0.0470 | Hit@50=0.2305 NDCG@50=0.0780\n",
      "  ✅ New best NDCG@10=0.04700 at epoch 24\n",
      "[Graph2] Epoch 25 | loss=0.1574 | time=38.5s | Hit@10=0.0874 NDCG@10=0.0473 | Hit@50=0.2324 NDCG@50=0.0786\n",
      "  ✅ New best NDCG@10=0.04734 at epoch 25\n",
      "[Graph2] Epoch 26 | loss=0.1552 | time=38.4s | Hit@10=0.0876 NDCG@10=0.0474 | Hit@50=0.2343 NDCG@50=0.0790\n",
      "  patience=1/5 (best 0.04734 @ epoch 25)\n",
      "[Graph2] Epoch 27 | loss=0.1533 | time=38.1s | Hit@10=0.0881 NDCG@10=0.0477 | Hit@50=0.2353 NDCG@50=0.0795\n",
      "  ✅ New best NDCG@10=0.04770 at epoch 27\n",
      "[Graph2] Epoch 28 | loss=0.1516 | time=38.5s | Hit@10=0.0888 NDCG@10=0.0481 | Hit@50=0.2370 NDCG@50=0.0800\n",
      "  ✅ New best NDCG@10=0.04807 at epoch 28\n",
      "[Graph2] Epoch 29 | loss=0.1498 | time=38.8s | Hit@10=0.0895 NDCG@10=0.0484 | Hit@50=0.2388 NDCG@50=0.0806\n",
      "  ✅ New best NDCG@10=0.04839 at epoch 29\n",
      "[Graph2] Epoch 30 | loss=0.1477 | time=37.9s | Hit@10=0.0902 NDCG@10=0.0488 | Hit@50=0.2399 NDCG@50=0.0810\n",
      "  ✅ New best NDCG@10=0.04878 at epoch 30\n",
      "\n",
      "Best epoch: 30 | best val NDCG@10: 0.048776859653511306\n",
      "\n",
      "TEST metrics (Graph2, best epoch):\n",
      "  Hit@10: 0.090340\n",
      "  NDCG@10: 0.048598\n",
      "  Hit@20: 0.136971\n",
      "  NDCG@20: 0.060308\n",
      "  Hit@50: 0.237612\n",
      "  NDCG@50: 0.080158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>Hit@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>Hit@20</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>Hit@50</th>\n",
       "      <th>NDCG@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.578549</td>\n",
       "      <td>36.884287</td>\n",
       "      <td>0.046144</td>\n",
       "      <td>0.026038</td>\n",
       "      <td>0.074572</td>\n",
       "      <td>0.033180</td>\n",
       "      <td>0.129275</td>\n",
       "      <td>0.043979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.406551</td>\n",
       "      <td>37.491725</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.027212</td>\n",
       "      <td>0.077681</td>\n",
       "      <td>0.034556</td>\n",
       "      <td>0.133694</td>\n",
       "      <td>0.045624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.370962</td>\n",
       "      <td>37.104557</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.029675</td>\n",
       "      <td>0.082793</td>\n",
       "      <td>0.036843</td>\n",
       "      <td>0.141953</td>\n",
       "      <td>0.048510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.338048</td>\n",
       "      <td>37.763722</td>\n",
       "      <td>0.057362</td>\n",
       "      <td>0.030990</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.038337</td>\n",
       "      <td>0.149275</td>\n",
       "      <td>0.050683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.315009</td>\n",
       "      <td>37.614854</td>\n",
       "      <td>0.058410</td>\n",
       "      <td>0.031737</td>\n",
       "      <td>0.089067</td>\n",
       "      <td>0.039446</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.052386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.296218</td>\n",
       "      <td>37.519017</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.032822</td>\n",
       "      <td>0.092588</td>\n",
       "      <td>0.040792</td>\n",
       "      <td>0.160193</td>\n",
       "      <td>0.054115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.275092</td>\n",
       "      <td>36.928283</td>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.034529</td>\n",
       "      <td>0.098431</td>\n",
       "      <td>0.043179</td>\n",
       "      <td>0.170624</td>\n",
       "      <td>0.057372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.253508</td>\n",
       "      <td>37.334149</td>\n",
       "      <td>0.067830</td>\n",
       "      <td>0.036599</td>\n",
       "      <td>0.104161</td>\n",
       "      <td>0.045712</td>\n",
       "      <td>0.180868</td>\n",
       "      <td>0.060809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.236324</td>\n",
       "      <td>38.984444</td>\n",
       "      <td>0.070883</td>\n",
       "      <td>0.038135</td>\n",
       "      <td>0.108712</td>\n",
       "      <td>0.047636</td>\n",
       "      <td>0.187779</td>\n",
       "      <td>0.063210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.223530</td>\n",
       "      <td>37.697372</td>\n",
       "      <td>0.072681</td>\n",
       "      <td>0.039156</td>\n",
       "      <td>0.111952</td>\n",
       "      <td>0.049021</td>\n",
       "      <td>0.193378</td>\n",
       "      <td>0.065038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.214169</td>\n",
       "      <td>37.676738</td>\n",
       "      <td>0.074160</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>0.066311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.206729</td>\n",
       "      <td>38.320253</td>\n",
       "      <td>0.075939</td>\n",
       "      <td>0.040728</td>\n",
       "      <td>0.116671</td>\n",
       "      <td>0.050927</td>\n",
       "      <td>0.200307</td>\n",
       "      <td>0.067395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.200224</td>\n",
       "      <td>37.486270</td>\n",
       "      <td>0.076857</td>\n",
       "      <td>0.041298</td>\n",
       "      <td>0.118019</td>\n",
       "      <td>0.051615</td>\n",
       "      <td>0.203641</td>\n",
       "      <td>0.068494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.194536</td>\n",
       "      <td>37.666221</td>\n",
       "      <td>0.077906</td>\n",
       "      <td>0.041865</td>\n",
       "      <td>0.119780</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.206375</td>\n",
       "      <td>0.069446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.189251</td>\n",
       "      <td>38.458510</td>\n",
       "      <td>0.079198</td>\n",
       "      <td>0.042493</td>\n",
       "      <td>0.121503</td>\n",
       "      <td>0.053093</td>\n",
       "      <td>0.210176</td>\n",
       "      <td>0.070589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.184307</td>\n",
       "      <td>37.905460</td>\n",
       "      <td>0.080321</td>\n",
       "      <td>0.043224</td>\n",
       "      <td>0.123282</td>\n",
       "      <td>0.053984</td>\n",
       "      <td>0.214165</td>\n",
       "      <td>0.071896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>37.113067</td>\n",
       "      <td>0.081202</td>\n",
       "      <td>0.043822</td>\n",
       "      <td>0.125155</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.217218</td>\n",
       "      <td>0.072979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.176320</td>\n",
       "      <td>37.146704</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>0.044566</td>\n",
       "      <td>0.126784</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>0.219971</td>\n",
       "      <td>0.074024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.172927</td>\n",
       "      <td>37.894037</td>\n",
       "      <td>0.083580</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>0.127889</td>\n",
       "      <td>0.056155</td>\n",
       "      <td>0.221918</td>\n",
       "      <td>0.074728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.169034</td>\n",
       "      <td>37.129270</td>\n",
       "      <td>0.084273</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.129818</td>\n",
       "      <td>0.056941</td>\n",
       "      <td>0.224147</td>\n",
       "      <td>0.075569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.166536</td>\n",
       "      <td>36.597780</td>\n",
       "      <td>0.085303</td>\n",
       "      <td>0.045992</td>\n",
       "      <td>0.130829</td>\n",
       "      <td>0.057416</td>\n",
       "      <td>0.225626</td>\n",
       "      <td>0.076152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.163964</td>\n",
       "      <td>37.798325</td>\n",
       "      <td>0.085921</td>\n",
       "      <td>0.046434</td>\n",
       "      <td>0.132870</td>\n",
       "      <td>0.058205</td>\n",
       "      <td>0.227911</td>\n",
       "      <td>0.076962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.161682</td>\n",
       "      <td>38.406807</td>\n",
       "      <td>0.086408</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.134106</td>\n",
       "      <td>0.058668</td>\n",
       "      <td>0.228585</td>\n",
       "      <td>0.077329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.159535</td>\n",
       "      <td>38.075611</td>\n",
       "      <td>0.086707</td>\n",
       "      <td>0.046997</td>\n",
       "      <td>0.135061</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.230458</td>\n",
       "      <td>0.077960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.157358</td>\n",
       "      <td>38.516763</td>\n",
       "      <td>0.087382</td>\n",
       "      <td>0.047339</td>\n",
       "      <td>0.135960</td>\n",
       "      <td>0.059536</td>\n",
       "      <td>0.232387</td>\n",
       "      <td>0.078568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.155185</td>\n",
       "      <td>38.356200</td>\n",
       "      <td>0.087606</td>\n",
       "      <td>0.047434</td>\n",
       "      <td>0.136934</td>\n",
       "      <td>0.059819</td>\n",
       "      <td>0.234278</td>\n",
       "      <td>0.079039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>38.132934</td>\n",
       "      <td>0.088074</td>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.137983</td>\n",
       "      <td>0.060225</td>\n",
       "      <td>0.235346</td>\n",
       "      <td>0.079457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.151635</td>\n",
       "      <td>38.541953</td>\n",
       "      <td>0.088767</td>\n",
       "      <td>0.048070</td>\n",
       "      <td>0.138488</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>0.237031</td>\n",
       "      <td>0.080011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.149805</td>\n",
       "      <td>38.780503</td>\n",
       "      <td>0.089479</td>\n",
       "      <td>0.048390</td>\n",
       "      <td>0.139481</td>\n",
       "      <td>0.060946</td>\n",
       "      <td>0.238792</td>\n",
       "      <td>0.080553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.147670</td>\n",
       "      <td>37.901701</td>\n",
       "      <td>0.090153</td>\n",
       "      <td>0.048777</td>\n",
       "      <td>0.140492</td>\n",
       "      <td>0.061401</td>\n",
       "      <td>0.239859</td>\n",
       "      <td>0.081015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss   time_sec    Hit@10   NDCG@10    Hit@20   NDCG@20  \\\n",
       "0       1  0.578549  36.884287  0.046144  0.026038  0.074572  0.033180   \n",
       "1       2  0.406551  37.491725  0.048485  0.027212  0.077681  0.034556   \n",
       "2       3  0.370962  37.104557  0.054141  0.029675  0.082793  0.036843   \n",
       "3       4  0.338048  37.763722  0.057362  0.030990  0.086614  0.038337   \n",
       "4       5  0.315009  37.614854  0.058410  0.031737  0.089067  0.039446   \n",
       "5       6  0.296218  37.519017  0.060751  0.032822  0.092588  0.040792   \n",
       "6       7  0.275092  36.928283  0.063898  0.034529  0.098431  0.043179   \n",
       "7       8  0.253508  37.334149  0.067830  0.036599  0.104161  0.045712   \n",
       "8       9  0.236324  38.984444  0.070883  0.038135  0.108712  0.047636   \n",
       "9      10  0.223530  37.697372  0.072681  0.039156  0.111952  0.049021   \n",
       "10     11  0.214169  37.676738  0.074160  0.039937  0.114124  0.049981   \n",
       "11     12  0.206729  38.320253  0.075939  0.040728  0.116671  0.050927   \n",
       "12     13  0.200224  37.486270  0.076857  0.041298  0.118019  0.051615   \n",
       "13     14  0.194536  37.666221  0.077906  0.041865  0.119780  0.052357   \n",
       "14     15  0.189251  38.458510  0.079198  0.042493  0.121503  0.053093   \n",
       "15     16  0.184307  37.905460  0.080321  0.043224  0.123282  0.053984   \n",
       "16     17  0.180200  37.113067  0.081202  0.043822  0.125155  0.054839   \n",
       "17     18  0.176320  37.146704  0.082550  0.044566  0.126784  0.055654   \n",
       "18     19  0.172927  37.894037  0.083580  0.045031  0.127889  0.056155   \n",
       "19     20  0.169034  37.129270  0.084273  0.045516  0.129818  0.056941   \n",
       "20     21  0.166536  36.597780  0.085303  0.045992  0.130829  0.057416   \n",
       "21     22  0.163964  37.798325  0.085921  0.046434  0.132870  0.058205   \n",
       "22     23  0.161682  38.406807  0.086408  0.046705  0.134106  0.058668   \n",
       "23     24  0.159535  38.075611  0.086707  0.046997  0.135061  0.059130   \n",
       "24     25  0.157358  38.516763  0.087382  0.047339  0.135960  0.059536   \n",
       "25     26  0.155185  38.356200  0.087606  0.047434  0.136934  0.059819   \n",
       "26     27  0.153300  38.132934  0.088074  0.047701  0.137983  0.060225   \n",
       "27     28  0.151635  38.541953  0.088767  0.048070  0.138488  0.060555   \n",
       "28     29  0.149805  38.780503  0.089479  0.048390  0.139481  0.060946   \n",
       "29     30  0.147670  37.901701  0.090153  0.048777  0.140492  0.061401   \n",
       "\n",
       "      Hit@50   NDCG@50  \n",
       "0   0.129275  0.043979  \n",
       "1   0.133694  0.045624  \n",
       "2   0.141953  0.048510  \n",
       "3   0.149275  0.050683  \n",
       "4   0.154800  0.052386  \n",
       "5   0.160193  0.054115  \n",
       "6   0.170624  0.057372  \n",
       "7   0.180868  0.060809  \n",
       "8   0.187779  0.063210  \n",
       "9   0.193378  0.065038  \n",
       "10  0.197067  0.066311  \n",
       "11  0.200307  0.067395  \n",
       "12  0.203641  0.068494  \n",
       "13  0.206375  0.069446  \n",
       "14  0.210176  0.070589  \n",
       "15  0.214165  0.071896  \n",
       "16  0.217218  0.072979  \n",
       "17  0.219971  0.074024  \n",
       "18  0.221918  0.074728  \n",
       "19  0.224147  0.075569  \n",
       "20  0.225626  0.076152  \n",
       "21  0.227911  0.076962  \n",
       "22  0.228585  0.077329  \n",
       "23  0.230458  0.077960  \n",
       "24  0.232387  0.078568  \n",
       "25  0.234278  0.079039  \n",
       "26  0.235346  0.079457  \n",
       "27  0.237031  0.080011  \n",
       "28  0.238792  0.080553  \n",
       "29  0.239859  0.081015  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 22: Train on Graph2 (full tag coverage) with early stopping (val NDCG@10)\n",
    "# - runs up to EPOCHS epochs but stops if no improvement\n",
    "# - keeps best model state in RAM\n",
    "# - at the end: evaluates on TEST using the best epoch\n",
    "\n",
    "EMB_DIM = 64\n",
    "NUM_LAYERS = 3\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "ACC_STEPS = 16\n",
    "EPOCHS = 30\n",
    "\n",
    "USER_BATCH_EVAL = 1024\n",
    "\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "print(\"Training config:\", {\n",
    "    \"EMB_DIM\": EMB_DIM, \"NUM_LAYERS\": NUM_LAYERS, \"LR\": LR,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE, \"ACC_STEPS\": ACC_STEPS, \"EPOCHS\": EPOCHS,\n",
    "    \"USER_BATCH_EVAL\": USER_BATCH_EVAL,\n",
    "    \"PATIENCE\": PATIENCE, \"MIN_DELTA\": MIN_DELTA\n",
    "})\n",
    "\n",
    "model2 = LightGCN(num_nodes=num_nodes2, emb_dim=EMB_DIM, num_layers=NUM_LAYERS, dropout=0.0).to(DEVICE)\n",
    "opt2 = Adam(model2.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_users = train_df[\"user_idx\"].to_numpy(dtype=np.int32)\n",
    "train_items = train_df[\"book_idx\"].to_numpy(dtype=np.int32)\n",
    "num_inter = len(train_users)\n",
    "perm = np.arange(num_inter)\n",
    "\n",
    "block_size = BATCH_SIZE * ACC_STEPS\n",
    "\n",
    "best_ndcg10 = -1.0\n",
    "best_epoch = 0\n",
    "best_state = None\n",
    "pat = 0\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model2.train()\n",
    "    np.random.shuffle(perm)\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    for block_start in range(0, num_inter, block_size):\n",
    "        all_emb = model2.propagate(A_norm2)\n",
    "        loss_block = 0.0\n",
    "\n",
    "        remaining = num_inter - block_start\n",
    "        n_inner = min(ACC_STEPS, int(np.ceil(remaining / BATCH_SIZE)))\n",
    "\n",
    "        for inner in range(n_inner):\n",
    "            start = block_start + inner * BATCH_SIZE\n",
    "            batch_idx = perm[start:start + BATCH_SIZE]\n",
    "            bu = train_users[batch_idx]\n",
    "            bi = train_items[batch_idx]\n",
    "            bn = sample_negatives_fast(bu, train_pos, B, K_try=20)\n",
    "\n",
    "            u_nodes = torch.from_numpy(bu.astype(np.int64) + user_offset2).to(DEVICE)\n",
    "            p_nodes = torch.from_numpy(bi.astype(np.int64) + book_offset2).to(DEVICE)\n",
    "            n_nodes = torch.from_numpy(bn.astype(np.int64) + book_offset2).to(DEVICE)\n",
    "\n",
    "            u_emb = all_emb[u_nodes]\n",
    "            p_emb = all_emb[p_nodes]\n",
    "            n_emb = all_emb[n_nodes]\n",
    "\n",
    "            loss_block = loss_block + bpr_loss(u_emb, p_emb, n_emb)\n",
    "\n",
    "        loss_block = loss_block / n_inner\n",
    "\n",
    "        opt2.zero_grad(set_to_none=True)\n",
    "        loss_block.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        epoch_loss += float(loss_block.item())\n",
    "        n_steps += 1\n",
    "\n",
    "        del all_emb, loss_block\n",
    "\n",
    "    # ---- val ----\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        all_emb_val = model2.propagate(A_norm2)\n",
    "\n",
    "    val_metrics = eval_ranking(\n",
    "        all_emb_val,\n",
    "        val_gt,\n",
    "        train_pos,\n",
    "        K_list=(10, 20, 50),\n",
    "        user_batch_size=USER_BATCH_EVAL,\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    row = {\"epoch\": epoch, \"loss\": epoch_loss / max(n_steps, 1), \"time_sec\": dt, **val_metrics}\n",
    "    history.append(row)\n",
    "\n",
    "    print(\n",
    "        f\"[Graph2] Epoch {epoch:02d} | loss={row['loss']:.4f} | time={dt:.1f}s | \"\n",
    "        f\"Hit@10={row['Hit@10']:.4f} NDCG@10={row['NDCG@10']:.4f} | \"\n",
    "        f\"Hit@50={row['Hit@50']:.4f} NDCG@50={row['NDCG@50']:.4f}\"\n",
    "    )\n",
    "\n",
    "    cur = row[\"NDCG@10\"]\n",
    "    if cur > best_ndcg10 + MIN_DELTA:\n",
    "        best_ndcg10 = cur\n",
    "        best_epoch = epoch\n",
    "        best_state = copy.deepcopy(model2.state_dict())\n",
    "        pat = 0\n",
    "        print(f\"  ✅ New best NDCG@10={best_ndcg10:.5f} at epoch {best_epoch}\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        print(f\"  patience={pat}/{PATIENCE} (best {best_ndcg10:.5f} @ epoch {best_epoch})\")\n",
    "\n",
    "    if pat >= PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "hist_df2 = pd.DataFrame(history)\n",
    "print(\"\\nBest epoch:\", best_epoch, \"| best val NDCG@10:\", best_ndcg10)\n",
    "\n",
    "# ---- restore best and evaluate on TEST ----\n",
    "if best_state is not None:\n",
    "    model2.load_state_dict(best_state)\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    all_emb_test = model2.propagate(A_norm2)\n",
    "\n",
    "test_metrics2 = eval_ranking(\n",
    "    all_emb_test,\n",
    "    test_gt,\n",
    "    train_pos,\n",
    "    K_list=(10, 20, 50),\n",
    "    user_batch_size=USER_BATCH_EVAL,\n",
    ")\n",
    "\n",
    "print(\"\\nTEST metrics (Graph2, best epoch):\")\n",
    "for k, v in test_metrics2.items():\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "hist_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c59c876-28f5-46b4-b90f-315bac431660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST metrics (Graph2 augmented user–book–tag):\n",
      "  Hit@10: 0.090340\n",
      "  NDCG@10: 0.048598\n",
      "  Hit@20: 0.136971\n",
      "  NDCG@20: 0.060308\n",
      "  Hit@50: 0.237612\n",
      "  NDCG@50: 0.080158\n"
     ]
    }
   ],
   "source": [
    "# Cell 23: Final evaluation on TEST (leave-one-out)\n",
    "# - compute ranking metrics on FULL TEST (1 GT item per user)\n",
    "# - filter already-seen train positives (train_pos)\n",
    "# - uses Graph2 best model and Graph2 adjacency A_norm2\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    all_emb_test = model2.propagate(A_norm2)\n",
    "\n",
    "test_metrics = eval_ranking(\n",
    "    all_emb_test,\n",
    "    test_gt,\n",
    "    train_pos,\n",
    "    K_list=(10, 20, 50),\n",
    "    user_batch_size=USER_BATCH_EVAL,\n",
    ")\n",
    "\n",
    "print(\"TEST metrics (Graph2 augmented user–book–tag):\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd7a1d90-938e-4ff9-9602-8b4c8338b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>baseline</th>\n",
       "      <th>graph2_test</th>\n",
       "      <th>delta_abs</th>\n",
       "      <th>delta_rel_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hit@10</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.090340</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>7.548169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDCG@10</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.048598</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>7.996178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hit@20</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.136971</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>6.179397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDCG@20</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.060308</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>5.804379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hit@50</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.237612</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>7.516695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NDCG@50</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.080158</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>6.877812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    metric  baseline  graph2_test  delta_abs  delta_rel_%\n",
       "0   Hit@10     0.084     0.090340   0.006340     7.548169\n",
       "1  NDCG@10     0.045     0.048598   0.003598     7.996178\n",
       "2   Hit@20     0.129     0.136971   0.007971     6.179397\n",
       "3  NDCG@20     0.057     0.060308   0.003308     5.804379\n",
       "4   Hit@50     0.221     0.237612   0.016612     7.516695\n",
       "5  NDCG@50     0.075     0.080158   0.005158     6.877812"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 24: Compare Graph2 TEST metrics vs baseline reference\n",
    "# - baseline_ref: your baseline user–book LightGCN + hard negatives\n",
    "# - show absolute deltas\n",
    "\n",
    "baseline_ref = {\n",
    "    \"Hit@10\": 0.084,\n",
    "    \"NDCG@10\": 0.045,\n",
    "    \"Hit@20\": 0.129,\n",
    "    \"NDCG@20\": 0.057,\n",
    "    \"Hit@50\": 0.221,\n",
    "    \"NDCG@50\": 0.075,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for m in [\"Hit@10\",\"NDCG@10\",\"Hit@20\",\"NDCG@20\",\"Hit@50\",\"NDCG@50\"]:\n",
    "    aug = float(test_metrics.get(m, np.nan))\n",
    "    base = float(baseline_ref.get(m, np.nan))\n",
    "    rows.append({\n",
    "        \"metric\": m,\n",
    "        \"baseline\": base,\n",
    "        \"graph2_test\": aug,\n",
    "        \"delta_abs\": aug - base,\n",
    "        \"delta_rel_%\": (aug - base) / base * 100 if base and not np.isnan(aug) else np.nan\n",
    "    })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows)\n",
    "cmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bbf5e-134c-4e15-b67d-832b239e39ea",
   "metadata": {},
   "source": [
    "## Conclusions — Graph2 (User–Book–Tag LightGCN)\n",
    "\n",
    "### What was done\n",
    "We extended the baseline **pure collaborative filtering** setup (user–book bipartite graph) by introducing **content-aware graph augmentation** using book tags from the GoodBooks dataset.\n",
    "\n",
    "The final graph structure is:\n",
    "\n",
    "user — book — tag\n",
    "\n",
    "\n",
    "Key properties:\n",
    "- **100% book coverage** via correct `goodreads_book_id → book_id` mapping\n",
    "- Tags filtered by minimum book frequency and capped per book\n",
    "- Homogeneous graph trained with **LightGCN + BPR loss**\n",
    "- Strict **leave-one-out split per user** (no leakage)\n",
    "- Ranking-based evaluation (Hit@K, NDCG@K)\n",
    "\n",
    "### Results (TEST set)\n",
    "\n",
    "| Metric     | Baseline (user–book) | Graph2 (user–book–tag) | Δ Absolute | Δ Relative |\n",
    "|------------|----------------------|-------------------------|------------|------------|\n",
    "| Hit@10     | 0.0840               | **0.0903**              | +0.0063    | +7.5%      |\n",
    "| NDCG@10    | 0.0450               | **0.0486**              | +0.0036    | +8.0%      |\n",
    "| Hit@20     | 0.1290               | **0.1370**              | +0.0080    | +6.2%      |\n",
    "| NDCG@20    | 0.0570               | **0.0603**              | +0.0033    | +5.8%      |\n",
    "| Hit@50     | 0.2210               | **0.2376**              | +0.0166    | +7.5%      |\n",
    "| NDCG@50    | 0.0750               | **0.0802**              | +0.0052    | +6.9%      |\n",
    "\n",
    "### Key observations\n",
    "- The **baseline ceiling was broken**: pure CF saturated at NDCG@10 ≈ 0.045, while Graph2 reached **≈ 0.049**.\n",
    "- Gains are **consistent across all K**, confirming that improvement is not noise.\n",
    "- The model continued improving up to ~30 epochs, indicating **effective information propagation** rather than overfitting.\n",
    "- Crucially, the improvement comes **not from architectural complexity**, but from **adding meaningful semantic relations to the graph**.\n",
    "\n",
    "### Why it worked\n",
    "- Tags act as **shared semantic hubs** connecting related books.\n",
    "- LightGCN efficiently propagates signals across `user → book → tag → book` paths.\n",
    "- Correct ID mapping and full coverage were essential — partial tag coverage previously nullified the effect.\n",
    "- This validates the core hypothesis:  \n",
    "  **graph enrichment is more impactful than model complexity for recommender systems.**\n",
    "\n",
    "### Limitations\n",
    "- Tag edges are currently **unweighted** (tag `count` is ignored).\n",
    "- The model remains **homogeneous** (node and edge types are not distinguished).\n",
    "- Textual and numerical book metadata from `books.csv` are not yet used.\n",
    "\n",
    "### Next steps\n",
    "In the next iteration, we will move toward a **fully hybrid recommender** by:\n",
    "1. Adding **weighted book–tag edges** based on tag frequency.\n",
    "2. Introducing **book–book similarity edges** derived from textual metadata (TF-IDF / SBERT).\n",
    "3. Exploring **heterogeneous GNN architectures** with typed relations.\n",
    "\n",
    "These steps aim to further improve ranking quality while preserving a clean, reproducible research pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf599c7a-f591-44ca-8648-15f1b465d1a4",
   "metadata": {},
   "source": [
    "## Graph3 — Fully Augmented Hybrid Graph Recommender\n",
    "\n",
    "### Motivation\n",
    "Previous experiments demonstrated that **graph enrichment is the primary driver of recommendation quality**, not architectural complexity alone.  \n",
    "After successfully improving over the pure collaborative filtering baseline using **user–book–tag augmentation (Graph2)**, the next step is to build a **fully hybrid graph** that incorporates *all available structured and textual signals* from the dataset.\n",
    "\n",
    "The goal of Graph3 is to move from:\n",
    "> *“collaborative + one semantic signal”*  \n",
    "to  \n",
    "> **“collaborative + multi-source content-aware reasoning.”**\n",
    "\n",
    "### Graph3: What changes compared to Graph2\n",
    "\n",
    "Graph3 integrates **all available information sources** into a single unified graph.\n",
    "\n",
    "#### 1. Interaction signal (unchanged)\n",
    "- **user ↔ book**\n",
    "- Derived only from **TRAIN interactions**\n",
    "- Leave-one-out per user is preserved\n",
    "- Prevents information leakage\n",
    "\n",
    "#### 2. Tag-based semantic signal (enhanced)\n",
    "- **book ↔ tag**\n",
    "- Edges are **weighted** using `log1p(tag_count)`\n",
    "- Provides stronger propagation through highly representative tags\n",
    "\n",
    "#### 3. Text-based semantic similarity (new)\n",
    "- **book ↔ book** edges\n",
    "- Built using **TF-IDF embeddings** of `title + authors`\n",
    "- Top-K nearest neighbors per book (cosine similarity)\n",
    "- Captures latent thematic similarity beyond explicit tags\n",
    "\n",
    "#### 4. Author signal (new)\n",
    "- **book ↔ author**\n",
    "- Allows recommendation propagation between books by the same or related authors\n",
    "\n",
    "#### 5. Language signal (new)\n",
    "- **book ↔ language**\n",
    "- Helps separate and propagate preferences across language clusters\n",
    "\n",
    "#### 6. Temporal signal (new)\n",
    "- **book ↔ year_bin**\n",
    "- Books are connected to coarse publication-era nodes\n",
    "- Enables temporal preference smoothing\n",
    "\n",
    "### Unified graph structure\n",
    "\n",
    "user ─ book ─ tag\n",
    "│\n",
    "├── book (similarity)\n",
    "│\n",
    "├── author\n",
    "│\n",
    "├── language\n",
    "│\n",
    "└── year_bin\n",
    "\n",
    "\n",
    "All nodes are embedded in a **single shared latent space** using LightGCN.\n",
    "\n",
    "### Model and training\n",
    "- **Model:** LightGCN\n",
    "- **Loss:** Bayesian Personalized Ranking (BPR)\n",
    "- **Negative sampling:** uniform with collision filtering\n",
    "- **Edge normalization:** weighted symmetric normalization\n",
    "- **Evaluation:** leave-one-out ranking metrics (Hit@K, NDCG@K)\n",
    "- **Early stopping:** validation NDCG@10\n",
    "\n",
    "This setup allows the model to **propagate user preference signals through multiple semantic paths** without introducing attention or heavy parameterization.\n",
    "\n",
    "### What we expect to learn from Graph3\n",
    "- Whether **multi-source graph enrichment** yields further improvements over Graph2\n",
    "- Which content signals contribute positively (tags vs similarity vs metadata)\n",
    "- Whether adding signals introduces noise or consistently improves ranking quality\n",
    "\n",
    "Graph3 also serves as a **strong foundation** for future work:\n",
    "- heterogeneous GNNs (typed edges)\n",
    "- SBERT-based similarity edges\n",
    "- feature-aware or attention-based models\n",
    "\n",
    "### Next steps\n",
    "Depending on Graph3 results, we will:\n",
    "1. Tune edge construction (weights, thresholds, top-K)\n",
    "2. Replace TF-IDF similarity with **SBERT embeddings**\n",
    "3. Move to **heterogeneous GNN architectures**\n",
    "4. Perform ablation studies to quantify each signal’s contribution\n",
    "\n",
    "At this stage, the project transitions from *collaborative filtering* to a **full hybrid graph recommender system**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92824b6b-8288-480a-a2fd-44aeac81ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports + reproducibility\n",
    "import os, gc, json, time, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6930152e-2ef0-4833-8214-963bbd0a2178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_RAW: D:\\ML\\GNN\\graph_recsys\\data_raw\n",
      "DATA_PROCESSED: D:\\ML\\GNN\\graph_recsys\\data_processed\\v2_proper\n",
      "ARTIFACTS: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Project paths\n",
    "PROJECT_ROOT = Path(r\"D:/ML/GNN/graph_recsys\")\n",
    "DATA_RAW = PROJECT_ROOT / \"data_raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\" / \"v2_proper\"\n",
    "ARTIFACTS = PROJECT_ROOT / \"artifacts\" / \"v2_proper\"\n",
    "\n",
    "for p in [PROJECT_ROOT, DATA_RAW, DATA_PROCESSED, ARTIFACTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"ARTIFACTS:\", ARTIFACTS)\n",
    "\n",
    "RAW_FILES = {\n",
    "    \"books\": \"books.csv\",\n",
    "    \"tags\": \"tags.csv\",\n",
    "    \"book_tags\": \"book_tags.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a006907-1bc9-44b2-9e03-cc96a548e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ui: (4926384, 2) int32\n",
      "val_ui: (53398, 2) int32\n",
      "test_ui: (53398, 2) int32\n",
      "U: 53398 B: 9999\n",
      "(4926384, 4) (53398, 4) (53398, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>book_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1796</td>\n",
       "      <td>0</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4691</td>\n",
       "      <td>0</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2063</td>\n",
       "      <td>0</td>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  user_idx  book_idx\n",
       "0        1      258         0       257\n",
       "1        1     1796         0      1795\n",
       "2        1     4691         0      4690\n",
       "3        1     2063         0      2062\n",
       "4        1       11         0        10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load LOO splits (idx-space) + mappings user2idx/book2idx (saved as pd.Series.to_csv)\n",
    "def load_series_mapping(path: Path) -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Loads mapping saved by: pd.Series(dict).to_csv(\"user2idx.csv\")\n",
    "    This produces 2 columns without headers. We parse robustly.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    # Typical shape: (n+1, 2) with header row if index name saved; handle both\n",
    "    # Remove rows where key is not int-like\n",
    "    df = df.dropna()\n",
    "    # If first row is non-numeric (e.g. index name), drop it\n",
    "    def is_intlike(x):\n",
    "        try:\n",
    "            int(x); return True\n",
    "        except: \n",
    "            return False\n",
    "    df = df[df[0].apply(is_intlike) & df[1].apply(is_intlike)].copy()\n",
    "    df[0] = df[0].astype(int)\n",
    "    df[1] = df[1].astype(int)\n",
    "    return dict(zip(df[0].values.tolist(), df[1].values.tolist()))\n",
    "\n",
    "splits_path = DATA_PROCESSED / \"splits_ui.npz\"\n",
    "user_map_path = DATA_PROCESSED / \"user2idx.csv\"\n",
    "book_map_path = DATA_PROCESSED / \"book2idx.csv\"\n",
    "\n",
    "for p in [splits_path, user_map_path, book_map_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "z = np.load(splits_path, allow_pickle=True)\n",
    "train_ui = z[\"train_ui\"].astype(np.int32)\n",
    "val_ui   = z[\"val_ui\"].astype(np.int32)\n",
    "test_ui  = z[\"test_ui\"].astype(np.int32)\n",
    "\n",
    "user2idx = load_series_mapping(user_map_path)   # {user_id: user_idx}\n",
    "book2idx = load_series_mapping(book_map_path)   # {book_id: book_idx}\n",
    "\n",
    "idx2user = {u: uid for uid, u in user2idx.items()}\n",
    "idx2book = {i: bid for bid, i in book2idx.items()}\n",
    "\n",
    "U = max(idx2user.keys()) + 1\n",
    "B = max(idx2book.keys()) + 1\n",
    "\n",
    "print(\"train_ui:\", train_ui.shape, train_ui.dtype)\n",
    "print(\"val_ui:\", val_ui.shape, val_ui.dtype)\n",
    "print(\"test_ui:\", test_ui.shape, test_ui.dtype)\n",
    "print(\"U:\", U, \"B:\", B)\n",
    "\n",
    "def ui_to_df(ui: np.ndarray) -> pd.DataFrame:\n",
    "    u = ui[:, 0].astype(int)\n",
    "    i = ui[:, 1].astype(int)\n",
    "    df = pd.DataFrame({\"user_idx\": u, \"book_idx\": i})\n",
    "    df[\"user_id\"] = df[\"user_idx\"].map(idx2user).astype(int)\n",
    "    df[\"book_id\"] = df[\"book_idx\"].map(idx2book).astype(int)\n",
    "    return df[[\"user_id\", \"book_id\", \"user_idx\", \"book_idx\"]]\n",
    "\n",
    "train_df = ui_to_df(train_ui)\n",
    "val_df   = ui_to_df(val_ui)\n",
    "test_df  = ui_to_df(test_ui)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ded897-b0d1-4362-a24f-436f43daacbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos users: 53398\n",
      "val_gt users: 53398 test_gt users: 53398\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build train positives + ground truth dicts (LOO)\n",
    "\n",
    "train_pos = defaultdict(set)\n",
    "for u, i in train_ui:\n",
    "    train_pos[int(u)].add(int(i))\n",
    "\n",
    "val_gt = {int(u): int(i) for u, i in val_ui}\n",
    "test_gt = {int(u): int(i) for u, i in test_ui}\n",
    "\n",
    "print(\"train_pos users:\", len(train_pos))\n",
    "print(\"val_gt users:\", len(val_gt), \"test_gt users:\", len(test_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99ce9ed-bfdc-4d7c-b26d-ea3d4a3d2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books_df: (9999, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>book_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>language_code</th>\n",
       "      <th>year_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the hunger games (the hunger games, #1) suzann...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2006-2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>harry potter and the sorcerer's stone (harry p...</td>\n",
       "      <td>eng</td>\n",
       "      <td>1991-2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>twilight (twilight, #1) stephenie meyer</td>\n",
       "      <td>en-US</td>\n",
       "      <td>1991-2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>to kill a mockingbird harper lee</td>\n",
       "      <td>eng</td>\n",
       "      <td>1951-1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>the great gatsby f. scott fitzgerald</td>\n",
       "      <td>eng</td>\n",
       "      <td>&lt;=1950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  book_idx                                               text  \\\n",
       "0        1         0  the hunger games (the hunger games, #1) suzann...   \n",
       "1        2         1  harry potter and the sorcerer's stone (harry p...   \n",
       "2        3         2            twilight (twilight, #1) stephenie meyer   \n",
       "3        4         3                   to kill a mockingbird harper lee   \n",
       "4        5         4               the great gatsby f. scott fitzgerald   \n",
       "\n",
       "  language_code   year_bin  \n",
       "0           eng  2006-2015  \n",
       "1           eng  1991-2005  \n",
       "2         en-US  1991-2005  \n",
       "3           eng  1951-1970  \n",
       "4           eng     <=1950  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Load books.csv and prepare content nodes (author/lang/year_bin)\n",
    "books_path = DATA_RAW / RAW_FILES[\"books\"]\n",
    "books_df = pd.read_csv(books_path)\n",
    "\n",
    "# Required for ID alignment with our mapping\n",
    "assert {\"book_id\", \"goodreads_book_id\"}.issubset(books_df.columns)\n",
    "\n",
    "# Keep only books in our catalog (book_id space)\n",
    "books_df[\"book_id\"] = books_df[\"book_id\"].astype(int)\n",
    "books_df = books_df[books_df[\"book_id\"].isin(book2idx.keys())].copy()\n",
    "books_df[\"book_idx\"] = books_df[\"book_id\"].map(book2idx).astype(int)\n",
    "\n",
    "# Text for similarity edges\n",
    "def safe_str(x): \n",
    "    return \"\" if pd.isna(x) else str(x)\n",
    "\n",
    "title_col = \"title\" if \"title\" in books_df.columns else None\n",
    "authors_col = \"authors\" if \"authors\" in books_df.columns else None\n",
    "\n",
    "books_df[\"text\"] = (\n",
    "    books_df[title_col].map(safe_str) if title_col else \"\"\n",
    ").astype(str) + \" \" + (\n",
    "    books_df[authors_col].map(safe_str) if authors_col else \"\"\n",
    ").astype(str)\n",
    "books_df[\"text\"] = books_df[\"text\"].str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Authors: split by comma\n",
    "if authors_col:\n",
    "    books_df[\"authors_list\"] = books_df[\"authors\"].fillna(\"\").apply(\n",
    "        lambda s: [a.strip() for a in str(s).split(\",\") if a.strip()]\n",
    "    )\n",
    "else:\n",
    "    books_df[\"authors_list\"] = [[] for _ in range(len(books_df))]\n",
    "\n",
    "# Language (optional)\n",
    "lang_col = \"language_code\" if \"language_code\" in books_df.columns else None\n",
    "if lang_col:\n",
    "    books_df[\"language_code\"] = books_df[\"language_code\"].fillna(\"unk\").astype(str)\n",
    "else:\n",
    "    books_df[\"language_code\"] = \"unk\"\n",
    "\n",
    "# Year (optional)\n",
    "year_col = \"original_publication_year\" if \"original_publication_year\" in books_df.columns else None\n",
    "if year_col:\n",
    "    books_df[\"year\"] = pd.to_numeric(books_df[year_col], errors=\"coerce\")\n",
    "else:\n",
    "    books_df[\"year\"] = np.nan\n",
    "\n",
    "# Year bins (coarse, robust)\n",
    "books_df[\"year_bin\"] = pd.cut(\n",
    "    books_df[\"year\"],\n",
    "    bins=[0, 1950, 1970, 1990, 2005, 2015, 2030],\n",
    "    labels=[\"<=1950\", \"1951-1970\", \"1971-1990\", \"1991-2005\", \"2006-2015\", \"2016+\"],\n",
    "    include_lowest=True\n",
    ").astype(str).fillna(\"unknown\")\n",
    "\n",
    "print(\"books_df:\", books_df.shape)\n",
    "books_df[[\"book_id\",\"book_idx\",\"text\",\"language_code\",\"year_bin\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5fda13-1117-4bf4-948b-53db29c9126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_tags_df: (999856, 4)\n",
      "unique books: 9999 unique tags: 34225\n",
      "Final tags T: 5014\n",
      "Coverage books with tag: 9999 / 9999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "      <th>count</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619294</th>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>50755</td>\n",
       "      <td>10.834785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619295</th>\n",
       "      <td>0</td>\n",
       "      <td>1271</td>\n",
       "      <td>35418</td>\n",
       "      <td>10.475004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619296</th>\n",
       "      <td>0</td>\n",
       "      <td>4937</td>\n",
       "      <td>25968</td>\n",
       "      <td>10.164659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619297</th>\n",
       "      <td>0</td>\n",
       "      <td>1725</td>\n",
       "      <td>13819</td>\n",
       "      <td>9.533872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619298</th>\n",
       "      <td>0</td>\n",
       "      <td>1455</td>\n",
       "      <td>12985</td>\n",
       "      <td>9.471627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_idx  tag_idx  count          w\n",
       "619294         0     1688  50755  10.834785\n",
       "619295         0     1271  35418  10.475004\n",
       "619296         0     4937  25968  10.164659\n",
       "619297         0     1725  13819   9.533872\n",
       "619298         0     1455  12985   9.471627"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: Load tags/book_tags and build weighted book-tag edges (book_id space)\n",
    "# - map goodreads_book_id -> book_id (via books.csv)\n",
    "# - filter tags by MIN_BOOK_FREQ\n",
    "# - keep TOP_TAGS_PER_BOOK per book by count\n",
    "# - edge weight = log1p(count) (safe)\n",
    "\n",
    "tags_df = pd.read_csv(DATA_RAW / RAW_FILES[\"tags\"])\n",
    "book_tags_raw = pd.read_csv(DATA_RAW / RAW_FILES[\"book_tags\"])\n",
    "\n",
    "assert {\"goodreads_book_id\",\"tag_id\",\"count\"}.issubset(book_tags_raw.columns)\n",
    "assert {\"tag_id\",\"tag_name\"}.issubset(tags_df.columns)\n",
    "\n",
    "# Robust numeric parsing (avoid hidden NaNs/strings issues)\n",
    "book_tags_raw[\"goodreads_book_id\"] = pd.to_numeric(book_tags_raw[\"goodreads_book_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "book_tags_raw[\"tag_id\"] = pd.to_numeric(book_tags_raw[\"tag_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "book_tags_raw[\"count\"] = pd.to_numeric(book_tags_raw[\"count\"], errors=\"coerce\")\n",
    "\n",
    "book_tags_raw = book_tags_raw.dropna(subset=[\"goodreads_book_id\",\"tag_id\",\"count\"]).copy()\n",
    "book_tags_raw[\"goodreads_book_id\"] = book_tags_raw[\"goodreads_book_id\"].astype(int)\n",
    "book_tags_raw[\"tag_id\"] = book_tags_raw[\"tag_id\"].astype(int)\n",
    "\n",
    "# Safety: ensure non-negative counts (avoid log1p issues)\n",
    "book_tags_raw[\"count\"] = book_tags_raw[\"count\"].fillna(0)\n",
    "book_tags_raw.loc[book_tags_raw[\"count\"] < 0, \"count\"] = 0\n",
    "book_tags_raw[\"count\"] = book_tags_raw[\"count\"].astype(int)\n",
    "\n",
    "# map goodreads_book_id -> book_id using books_df\n",
    "g2b = dict(zip(books_df[\"goodreads_book_id\"].astype(int), books_df[\"book_id\"].astype(int)))\n",
    "book_tags_raw[\"book_id\"] = book_tags_raw[\"goodreads_book_id\"].map(g2b)\n",
    "book_tags_raw = book_tags_raw.dropna(subset=[\"book_id\"]).copy()\n",
    "book_tags_raw[\"book_id\"] = book_tags_raw[\"book_id\"].astype(int)\n",
    "\n",
    "# filter to our catalog book_ids\n",
    "book_tags_df = book_tags_raw[book_tags_raw[\"book_id\"].isin(book2idx.keys())].copy()\n",
    "\n",
    "print(\"book_tags_df:\", book_tags_df.shape)\n",
    "print(\"unique books:\", book_tags_df[\"book_id\"].nunique(), \"unique tags:\", book_tags_df[\"tag_id\"].nunique())\n",
    "\n",
    "# Filtering + top tags per book\n",
    "MIN_BOOK_FREQ = 10\n",
    "TOP_TAGS_PER_BOOK = 50\n",
    "\n",
    "tag_book_freq = book_tags_df.groupby(\"tag_id\")[\"book_id\"].nunique().sort_values(ascending=False)\n",
    "eligible_tags = tag_book_freq[tag_book_freq >= MIN_BOOK_FREQ].index\n",
    "bt = book_tags_df[book_tags_df[\"tag_id\"].isin(eligible_tags)].copy()\n",
    "\n",
    "bt = (\n",
    "    bt.sort_values([\"book_id\",\"count\"], ascending=[True, False])\n",
    "      .groupby(\"book_id\", as_index=False)\n",
    "      .head(TOP_TAGS_PER_BOOK)\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# map to idx\n",
    "bt[\"book_idx\"] = bt[\"book_id\"].map(book2idx).astype(int)\n",
    "\n",
    "# local tag vocab\n",
    "unique_tag_ids = np.sort(bt[\"tag_id\"].unique())\n",
    "tag2idx = {int(t): i for i, t in enumerate(unique_tag_ids)}\n",
    "T = len(tag2idx)\n",
    "bt[\"tag_idx\"] = bt[\"tag_id\"].map(tag2idx).astype(int)\n",
    "\n",
    "# weight: log1p(count)\n",
    "bt[\"w\"] = np.log1p(bt[\"count\"].astype(float))\n",
    "assert np.isfinite(bt[\"w\"]).all(), \"Non-finite weights detected in tag edges\"\n",
    "\n",
    "print(\"Final tags T:\", T)\n",
    "print(\"Coverage books with tag:\", bt[\"book_idx\"].nunique(), \"/\", B)\n",
    "bt[[\"book_idx\",\"tag_idx\",\"count\",\"w\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac76deb-8dd1-4ae3-89bb-3792d2cb0f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book-book sim edges: 265581 | mean sim: 0.27009127\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Book–Book similarity edges via TF-IDF (title+authors) -> kNN\n",
    "# - Adds semantic edges between books: book <-> book\n",
    "# - Weight = cosine similarity\n",
    "\n",
    "TOPK_SIM = 30  # per book\n",
    "MIN_SIM = 0.10  # prune weak similarities\n",
    "\n",
    "corpus = books_df.sort_values(\"book_idx\")[\"text\"].tolist()\n",
    "assert len(corpus) == B, f\"Expected {B} books in corpus, got {len(corpus)}\"\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_features=200000,\n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "X = tfidf.fit_transform(corpus)  # sparse [B, V]\n",
    "\n",
    "# cosine kNN in sparse space\n",
    "nnbrs = NearestNeighbors(n_neighbors=TOPK_SIM + 1, metric=\"cosine\", algorithm=\"brute\")\n",
    "nnbrs.fit(X)\n",
    "dist, idx = nnbrs.kneighbors(X, return_distance=True)\n",
    "\n",
    "# build edges excluding self (first neighbor)\n",
    "src = []\n",
    "dst = []\n",
    "w = []\n",
    "for i in range(B):\n",
    "    for j, d in zip(idx[i, 1:], dist[i, 1:]):\n",
    "        sim = 1.0 - float(d)\n",
    "        if sim >= MIN_SIM:\n",
    "            src.append(i)\n",
    "            dst.append(int(j))\n",
    "            w.append(sim)\n",
    "\n",
    "src = np.array(src, dtype=np.int64)\n",
    "dst = np.array(dst, dtype=np.int64)\n",
    "w = np.array(w, dtype=np.float32)\n",
    "\n",
    "print(\"book-book sim edges:\", len(src), \"| mean sim:\", w.mean() if len(w) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7450f3-de63-4f02-9d81-cee81492fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=53398 B=9999 T=5014 A=5841 L=26 Y=7 | num_nodes=74285\n",
      "Offsets: {'user': 0, 'book': 53398, 'tag': 63397, 'author': 68411, 'lang': 74252, 'year': 74278}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Unified node index space for Graph3 (all signals)\n",
    "# users:   [0 .. U)\n",
    "# books:   [U .. U+B)\n",
    "# tags:    [U+B .. U+B+T)\n",
    "# authors: [..]\n",
    "# langs:   [..]\n",
    "# yearbin: [..]\n",
    "\n",
    "user_offset = 0\n",
    "book_offset = U\n",
    "tag_offset = U + B\n",
    "author_offset = tag_offset + T\n",
    "\n",
    "# author vocab\n",
    "all_authors = sorted({a for lst in books_df[\"authors_list\"] for a in lst})\n",
    "author2idx = {a: i for i, a in enumerate(all_authors)}\n",
    "A = len(author2idx)\n",
    "\n",
    "lang_offset = author_offset + A\n",
    "langs = sorted(books_df[\"language_code\"].astype(str).unique().tolist())\n",
    "lang2idx = {l: i for i, l in enumerate(langs)}\n",
    "L = len(lang2idx)\n",
    "\n",
    "year_offset = lang_offset + L\n",
    "years = sorted(books_df[\"year_bin\"].astype(str).unique().tolist())\n",
    "year2idx = {y: i for i, y in enumerate(years)}\n",
    "Y = len(year2idx)\n",
    "\n",
    "num_nodes = U + B + T + A + L + Y\n",
    "\n",
    "print(f\"U={U} B={B} T={T} A={A} L={L} Y={Y} | num_nodes={num_nodes}\")\n",
    "print(\"Offsets:\", dict(user=user_offset, book=book_offset, tag=tag_offset,\n",
    "                      author=author_offset, lang=lang_offset, year=year_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441ec3d2-e0c1-491f-b735-b18faf581843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user-book edges.\n",
      "Added book-tag edges.\n",
      "Added book-book similarity edges.\n",
      "Added book-author edges.\n",
      "Added book-lang edges.\n",
      "Added book-yearbin edges.\n",
      "edge_index: torch.Size([2, 11450076]) edge_w: torch.Size([11450076])\n",
      "node id range: 0 74284\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Build Graph3 edges (weighted)\n",
    "# Edges:\n",
    "# - user<->book from TRAIN: weight=1\n",
    "# - book<->tag: weight=log1p(count)\n",
    "# - book<->book_sim: weight=cosine sim\n",
    "# - book<->author: weight=1\n",
    "# - book<->lang: weight=1\n",
    "# - book<->yearbin: weight=1\n",
    "\n",
    "edge_src = []\n",
    "edge_dst = []\n",
    "edge_w = []\n",
    "\n",
    "def add_undirected(u, v, w=1.0):\n",
    "    edge_src.append(u); edge_dst.append(v); edge_w.append(w)\n",
    "    edge_src.append(v); edge_dst.append(u); edge_w.append(w)\n",
    "\n",
    "# 1) user-book (TRAIN only)\n",
    "for u, i in train_ui:\n",
    "    un = user_offset + int(u)\n",
    "    bn = book_offset + int(i)\n",
    "    add_undirected(un, bn, 1.0)\n",
    "\n",
    "print(\"Added user-book edges.\")\n",
    "\n",
    "# 2) book-tag weighted\n",
    "for r in bt.itertuples(index=False):\n",
    "    bidx = int(r.book_idx)\n",
    "    tidx = int(r.tag_idx)\n",
    "    wgt = float(r.w)\n",
    "    bn = book_offset + bidx\n",
    "    tn = tag_offset + tidx\n",
    "    add_undirected(bn, tn, wgt)\n",
    "\n",
    "print(\"Added book-tag edges.\")\n",
    "\n",
    "# 3) book-book similarity weighted\n",
    "for bi, bj, ww in zip(src, dst, w):\n",
    "    b1 = book_offset + int(bi)\n",
    "    b2 = book_offset + int(bj)\n",
    "    add_undirected(b1, b2, float(ww))\n",
    "\n",
    "print(\"Added book-book similarity edges.\")\n",
    "\n",
    "# 4) book-author\n",
    "for r in books_df.itertuples(index=False):\n",
    "    bidx = int(r.book_idx)\n",
    "    bn = book_offset + bidx\n",
    "    for a in r.authors_list:\n",
    "        an = author_offset + author2idx[a]\n",
    "        add_undirected(bn, an, 1.0)\n",
    "\n",
    "print(\"Added book-author edges.\")\n",
    "\n",
    "# 5) book-lang\n",
    "for r in books_df.itertuples(index=False):\n",
    "    bidx = int(r.book_idx)\n",
    "    bn = book_offset + bidx\n",
    "    ln = lang_offset + lang2idx[str(r.language_code)]\n",
    "    add_undirected(bn, ln, 1.0)\n",
    "\n",
    "print(\"Added book-lang edges.\")\n",
    "\n",
    "# 6) book-yearbin\n",
    "for r in books_df.itertuples(index=False):\n",
    "    bidx = int(r.book_idx)\n",
    "    bn = book_offset + bidx\n",
    "    yn = year_offset + year2idx[str(r.year_bin)]\n",
    "    add_undirected(bn, yn, 1.0)\n",
    "\n",
    "print(\"Added book-yearbin edges.\")\n",
    "\n",
    "edge_src = torch.tensor(edge_src, dtype=torch.long)\n",
    "edge_dst = torch.tensor(edge_dst, dtype=torch.long)\n",
    "edge_w = torch.tensor(edge_w, dtype=torch.float32)\n",
    "\n",
    "edge_index = torch.stack([edge_src, edge_dst], dim=0)\n",
    "print(\"edge_index:\", edge_index.shape, \"edge_w:\", edge_w.shape)\n",
    "print(\"node id range:\", int(edge_index.min()), int(edge_index.max()))\n",
    "assert edge_index.min() >= 0 and edge_index.max() < num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "647ab8f1-be01-41ae-b158-f314f7421e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm: (74285, 74285) nnz: 11260793\n",
      "deg stats: 0.6931471824645996 185.75030517578125 83970.375\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Build weighted normalized adjacency A_norm for Graph3\n",
    "# - A is weighted sparse adjacency\n",
    "# - Normalize with symmetric norm: D^{-1/2} A D^{-1/2}\n",
    "\n",
    "A = torch.sparse_coo_tensor(edge_index, edge_w, (num_nodes, num_nodes)).coalesce()\n",
    "deg = torch.sparse.sum(A, dim=1).to_dense()\n",
    "deg_inv_sqrt = torch.pow(deg.clamp(min=1e-12), -0.5)\n",
    "\n",
    "row, col = A.indices()\n",
    "norm_val = deg_inv_sqrt[row] * A.values() * deg_inv_sqrt[col]\n",
    "A_norm = torch.sparse_coo_tensor(A.indices(), norm_val, A.size()).coalesce().to(DEVICE)\n",
    "\n",
    "print(\"A_norm:\", tuple(A_norm.shape), \"nnz:\", A_norm._nnz())\n",
    "print(\"deg stats:\", float(deg.min()), float(deg.mean()), float(deg.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d479af-feff-45b5-8e2f-e6c31f5e8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm values: min/mean/max = 0.0 0.0032131632324308157 0.117741659283638\n",
      "NaN: False | Inf: False\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for A_norm values\n",
    "vals = A_norm.values()\n",
    "print(\"A_norm values: min/mean/max =\", float(vals.min()), float(vals.mean()), float(vals.max()))\n",
    "print(\"NaN:\", torch.isnan(vals).any().item(), \"| Inf:\", torch.isinf(vals).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8220252d-6851-47a6-8b6f-bc3f4b2f90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + losses + eval ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: LightGCN + BPR + negative sampler + ranking eval\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.normal_(self.emb.weight, std=0.01)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def propagate(self, A_norm: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = self.emb.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        for _ in range(self.num_layers):\n",
    "            x = torch.sparse.mm(A_norm, x)\n",
    "            out = out + x\n",
    "        out = out / (self.num_layers + 1)\n",
    "        return out\n",
    "\n",
    "def bpr_loss(u, p, n):\n",
    "    pos = (u * p).sum(dim=1)\n",
    "    neg = (u * n).sum(dim=1)\n",
    "    return -torch.log(torch.sigmoid(pos - neg) + 1e-12).mean()\n",
    "\n",
    "def sample_negatives_fast(users_np: np.ndarray, train_pos: dict, num_items: int, K_try: int = 20) -> np.ndarray:\n",
    "    # uniform negatives with retries; fast enough for B=9999\n",
    "    neg = np.random.randint(0, num_items, size=len(users_np), dtype=np.int32)\n",
    "    for _ in range(K_try):\n",
    "        bad = np.array([neg[i] in train_pos[int(u)] for i, u in enumerate(users_np)], dtype=bool)\n",
    "        if not bad.any():\n",
    "            break\n",
    "        neg[bad] = np.random.randint(0, num_items, size=bad.sum(), dtype=np.int32)\n",
    "    return neg\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ranking(all_emb: torch.Tensor, gt: dict[int, int], train_pos: dict, K_list=(10,20,50), user_batch_size=1024):\n",
    "    # Only user/book embeddings are used for scoring\n",
    "    user_emb = all_emb[user_offset:user_offset + U].to(DEVICE)\n",
    "    item_emb = all_emb[book_offset:book_offset + B].to(DEVICE)\n",
    "\n",
    "    users = np.array(sorted(gt.keys()), dtype=np.int32)\n",
    "    hits = {K: 0 for K in K_list}\n",
    "    ndcgs = {K: 0.0 for K in K_list}\n",
    "    maxK = max(K_list)\n",
    "\n",
    "    for start in range(0, len(users), user_batch_size):\n",
    "        batch_users = users[start:start + user_batch_size]\n",
    "        bu = torch.from_numpy(batch_users.astype(np.int64)).to(DEVICE)\n",
    "\n",
    "        scores = user_emb[bu] @ item_emb.t()  # [batch, B]\n",
    "\n",
    "        # filter train positives\n",
    "        for row, u in enumerate(batch_users):\n",
    "            seen = train_pos.get(int(u), None)\n",
    "            if seen:\n",
    "                seen_idx = torch.tensor(list(seen), device=DEVICE, dtype=torch.long)\n",
    "                scores[row, seen_idx] = -1e9\n",
    "\n",
    "        topk = torch.topk(scores, k=maxK, dim=1).indices.cpu().numpy()\n",
    "\n",
    "        for row, u in enumerate(batch_users):\n",
    "            true_i = int(gt[int(u)])\n",
    "            rank = topk[row]\n",
    "            pos = np.where(rank == true_i)[0]\n",
    "            for K in K_list:\n",
    "                if len(pos) > 0 and pos[0] < K:\n",
    "                    hits[K] += 1\n",
    "                    ndcgs[K] += 1.0 / np.log2(pos[0] + 2.0)\n",
    "\n",
    "    n = len(users)\n",
    "    return {f\"Hit@{K}\": hits[K]/n for K in K_list} | {f\"NDCG@{K}\": ndcgs[K]/n for K in K_list}\n",
    "\n",
    "print(\"Model + losses + eval ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b9da675-4bb6-416b-8cc8-9d43136169c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config: {'EMB_DIM': 64, 'NUM_LAYERS': 3, 'LR': 0.002, 'WEIGHT_DECAY': 0.0, 'BATCH_SIZE': 4096, 'ACC_STEPS': 16, 'EPOCHS': 40, 'USER_BATCH_EVAL': 1024, 'PATIENCE': 6, 'MIN_DELTA': 0.0001}\n",
      "[Graph3-STABLE] Epoch 01 | loss=0.5915 | time=343.6s | Hit@10=0.0457 NDCG@10=0.0257 | Hit@50=0.1290 NDCG@50=0.0437\n",
      "New best NDCG@10=0.02571 at epoch 1\n",
      "[Graph3-STABLE] Epoch 02 | loss=0.4094 | time=343.8s | Hit@10=0.0484 NDCG@10=0.0268 | Hit@50=0.1324 NDCG@50=0.0449\n",
      "New best NDCG@10=0.02684 at epoch 2\n",
      "[Graph3-STABLE] Epoch 03 | loss=0.3775 | time=341.9s | Hit@10=0.0530 NDCG@10=0.0289 | Hit@50=0.1396 NDCG@50=0.0475\n",
      "New best NDCG@10=0.02891 at epoch 3\n",
      "[Graph3-STABLE] Epoch 04 | loss=0.3455 | time=341.6s | Hit@10=0.0566 NDCG@10=0.0306 | Hit@50=0.1474 NDCG@50=0.0500\n",
      "New best NDCG@10=0.03058 at epoch 4\n",
      "[Graph3-STABLE] Epoch 05 | loss=0.3191 | time=337.7s | Hit@10=0.0593 NDCG@10=0.0319 | Hit@50=0.1539 NDCG@50=0.0521\n",
      "New best NDCG@10=0.03190 at epoch 5\n",
      "[Graph3-STABLE] Epoch 06 | loss=0.2958 | time=338.2s | Hit@10=0.0611 NDCG@10=0.0331 | Hit@50=0.1611 NDCG@50=0.0545\n",
      "New best NDCG@10=0.03308 at epoch 6\n",
      "[Graph3-STABLE] Epoch 07 | loss=0.2724 | time=336.6s | Hit@10=0.0642 NDCG@10=0.0347 | Hit@50=0.1712 NDCG@50=0.0576\n",
      "New best NDCG@10=0.03468 at epoch 7\n",
      "[Graph3-STABLE] Epoch 08 | loss=0.2509 | time=338.9s | Hit@10=0.0682 NDCG@10=0.0367 | Hit@50=0.1819 NDCG@50=0.0610\n",
      "New best NDCG@10=0.03668 at epoch 8\n",
      "[Graph3-STABLE] Epoch 09 | loss=0.2343 | time=337.9s | Hit@10=0.0709 NDCG@10=0.0381 | Hit@50=0.1884 NDCG@50=0.0633\n",
      "New best NDCG@10=0.03812 at epoch 9\n",
      "[Graph3-STABLE] Epoch 10 | loss=0.2217 | time=342.9s | Hit@10=0.0733 NDCG@10=0.0393 | Hit@50=0.1941 NDCG@50=0.0652\n",
      "New best NDCG@10=0.03930 at epoch 10\n",
      "[Graph3-STABLE] Epoch 11 | loss=0.2125 | time=341.2s | Hit@10=0.0747 NDCG@10=0.0399 | Hit@50=0.1979 NDCG@50=0.0663\n",
      "New best NDCG@10=0.03993 at epoch 11\n",
      "[Graph3-STABLE] Epoch 12 | loss=0.2055 | time=338.4s | Hit@10=0.0761 NDCG@10=0.0407 | Hit@50=0.2012 NDCG@50=0.0674\n",
      "New best NDCG@10=0.04067 at epoch 12\n",
      "[Graph3-STABLE] Epoch 13 | loss=0.1989 | time=338.2s | Hit@10=0.0764 NDCG@10=0.0411 | Hit@50=0.2038 NDCG@50=0.0684\n",
      "New best NDCG@10=0.04108 at epoch 13\n",
      "[Graph3-STABLE] Epoch 14 | loss=0.1933 | time=337.8s | Hit@10=0.0777 NDCG@10=0.0416 | Hit@50=0.2071 NDCG@50=0.0694\n",
      "New best NDCG@10=0.04165 at epoch 14\n",
      "[Graph3-STABLE] Epoch 15 | loss=0.1888 | time=338.6s | Hit@10=0.0790 NDCG@10=0.0424 | Hit@50=0.2097 NDCG@50=0.0704\n",
      "New best NDCG@10=0.04239 at epoch 15\n",
      "[Graph3-STABLE] Epoch 16 | loss=0.1839 | time=337.7s | Hit@10=0.0795 NDCG@10=0.0429 | Hit@50=0.2119 NDCG@50=0.0713\n",
      "New best NDCG@10=0.04287 at epoch 16\n",
      "[Graph3-STABLE] Epoch 17 | loss=0.1797 | time=343.3s | Hit@10=0.0810 NDCG@10=0.0437 | Hit@50=0.2151 NDCG@50=0.0725\n",
      "New best NDCG@10=0.04371 at epoch 17\n",
      "[Graph3-STABLE] Epoch 18 | loss=0.1759 | time=342.6s | Hit@10=0.0818 NDCG@10=0.0442 | Hit@50=0.2183 NDCG@50=0.0735\n",
      "New best NDCG@10=0.04420 at epoch 18\n",
      "[Graph3-STABLE] Epoch 19 | loss=0.1722 | time=347.1s | Hit@10=0.0828 NDCG@10=0.0448 | Hit@50=0.2211 NDCG@50=0.0745\n",
      "New best NDCG@10=0.04479 at epoch 19\n",
      "[Graph3-STABLE] Epoch 20 | loss=0.1692 | time=345.6s | Hit@10=0.0836 NDCG@10=0.0453 | Hit@50=0.2235 NDCG@50=0.0754\n",
      "New best NDCG@10=0.04530 at epoch 20\n",
      "[Graph3-STABLE] Epoch 21 | loss=0.1663 | time=344.9s | Hit@10=0.0845 NDCG@10=0.0457 | Hit@50=0.2258 NDCG@50=0.0761\n",
      "New best NDCG@10=0.04570 at epoch 21\n",
      "[Graph3-STABLE] Epoch 22 | loss=0.1633 | time=344.3s | Hit@10=0.0857 NDCG@10=0.0463 | Hit@50=0.2282 NDCG@50=0.0769\n",
      "New best NDCG@10=0.04626 at epoch 22\n",
      "[Graph3-STABLE] Epoch 23 | loss=0.1606 | time=345.0s | Hit@10=0.0862 NDCG@10=0.0466 | Hit@50=0.2297 NDCG@50=0.0775\n",
      "New best NDCG@10=0.04661 at epoch 23\n",
      "[Graph3-STABLE] Epoch 24 | loss=0.1582 | time=343.4s | Hit@10=0.0868 NDCG@10=0.0469 | Hit@50=0.2318 NDCG@50=0.0781\n",
      "New best NDCG@10=0.04687 at epoch 24\n",
      "[Graph3-STABLE] Epoch 25 | loss=0.1557 | time=358.6s | Hit@10=0.0881 NDCG@10=0.0475 | Hit@50=0.2332 NDCG@50=0.0787\n",
      "New best NDCG@10=0.04745 at epoch 25\n",
      "[Graph3-STABLE] Epoch 26 | loss=0.1540 | time=354.9s | Hit@10=0.0886 NDCG@10=0.0478 | Hit@50=0.2344 NDCG@50=0.0792\n",
      "New best NDCG@10=0.04784 at epoch 26\n",
      "[Graph3-STABLE] Epoch 27 | loss=0.1515 | time=345.7s | Hit@10=0.0893 NDCG@10=0.0480 | Hit@50=0.2358 NDCG@50=0.0796\n",
      "New best NDCG@10=0.04804 at epoch 27\n",
      "[Graph3-STABLE] Epoch 28 | loss=0.1494 | time=345.0s | Hit@10=0.0900 NDCG@10=0.0484 | Hit@50=0.2380 NDCG@50=0.0802\n",
      "New best NDCG@10=0.04838 at epoch 28\n",
      "[Graph3-STABLE] Epoch 29 | loss=0.1476 | time=347.2s | Hit@10=0.0903 NDCG@10=0.0486 | Hit@50=0.2393 NDCG@50=0.0807\n",
      "New best NDCG@10=0.04862 at epoch 29\n",
      "[Graph3-STABLE] Epoch 30 | loss=0.1455 | time=341.8s | Hit@10=0.0913 NDCG@10=0.0492 | Hit@50=0.2407 NDCG@50=0.0814\n",
      "New best NDCG@10=0.04919 at epoch 30\n",
      "[Graph3-STABLE] Epoch 31 | loss=0.1434 | time=343.7s | Hit@10=0.0917 NDCG@10=0.0495 | Hit@50=0.2431 NDCG@50=0.0822\n",
      "New best NDCG@10=0.04955 at epoch 31\n",
      "[Graph3-STABLE] Epoch 32 | loss=0.1416 | time=344.4s | Hit@10=0.0924 NDCG@10=0.0499 | Hit@50=0.2444 NDCG@50=0.0826\n",
      "New best NDCG@10=0.04986 at epoch 32\n",
      "[Graph3-STABLE] Epoch 33 | loss=0.1394 | time=345.6s | Hit@10=0.0935 NDCG@10=0.0505 | Hit@50=0.2459 NDCG@50=0.0833\n",
      "New best NDCG@10=0.05050 at epoch 33\n",
      "[Graph3-STABLE] Epoch 34 | loss=0.1377 | time=343.7s | Hit@10=0.0940 NDCG@10=0.0508 | Hit@50=0.2470 NDCG@50=0.0838\n",
      "New best NDCG@10=0.05080 at epoch 34\n",
      "[Graph3-STABLE] Epoch 35 | loss=0.1363 | time=346.3s | Hit@10=0.0945 NDCG@10=0.0512 | Hit@50=0.2481 NDCG@50=0.0843\n",
      "New best NDCG@10=0.05116 at epoch 35\n",
      "[Graph3-STABLE] Epoch 36 | loss=0.1344 | time=347.0s | Hit@10=0.0951 NDCG@10=0.0516 | Hit@50=0.2498 NDCG@50=0.0850\n",
      "New best NDCG@10=0.05162 at epoch 36\n",
      "[Graph3-STABLE] Epoch 37 | loss=0.1333 | time=347.3s | Hit@10=0.0957 NDCG@10=0.0520 | Hit@50=0.2502 NDCG@50=0.0853\n",
      "New best NDCG@10=0.05197 at epoch 37\n",
      "[Graph3-STABLE] Epoch 38 | loss=0.1312 | time=348.2s | Hit@10=0.0967 NDCG@10=0.0523 | Hit@50=0.2516 NDCG@50=0.0857\n",
      "New best NDCG@10=0.05231 at epoch 38\n",
      "[Graph3-STABLE] Epoch 39 | loss=0.1301 | time=346.4s | Hit@10=0.0976 NDCG@10=0.0527 | Hit@50=0.2534 NDCG@50=0.0862\n",
      "New best NDCG@10=0.05267 at epoch 39\n",
      "[Graph3-STABLE] Epoch 40 | loss=0.1284 | time=345.2s | Hit@10=0.0981 NDCG@10=0.0531 | Hit@50=0.2547 NDCG@50=0.0868\n",
      "New best NDCG@10=0.05305 at epoch 40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>Hit@10</th>\n",
       "      <th>Hit@20</th>\n",
       "      <th>Hit@50</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>NDCG@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.143408</td>\n",
       "      <td>343.682516</td>\n",
       "      <td>0.091726</td>\n",
       "      <td>0.142908</td>\n",
       "      <td>0.243099</td>\n",
       "      <td>0.049546</td>\n",
       "      <td>0.062406</td>\n",
       "      <td>0.082171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.141633</td>\n",
       "      <td>344.392361</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.143376</td>\n",
       "      <td>0.244372</td>\n",
       "      <td>0.049864</td>\n",
       "      <td>0.062682</td>\n",
       "      <td>0.082625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.139377</td>\n",
       "      <td>345.608465</td>\n",
       "      <td>0.093505</td>\n",
       "      <td>0.144519</td>\n",
       "      <td>0.245889</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>0.063297</td>\n",
       "      <td>0.083307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.137745</td>\n",
       "      <td>343.653309</td>\n",
       "      <td>0.093955</td>\n",
       "      <td>0.145511</td>\n",
       "      <td>0.247013</td>\n",
       "      <td>0.050797</td>\n",
       "      <td>0.063747</td>\n",
       "      <td>0.083777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.136299</td>\n",
       "      <td>346.263726</td>\n",
       "      <td>0.094517</td>\n",
       "      <td>0.146391</td>\n",
       "      <td>0.248118</td>\n",
       "      <td>0.051158</td>\n",
       "      <td>0.064186</td>\n",
       "      <td>0.084266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.134399</td>\n",
       "      <td>347.004332</td>\n",
       "      <td>0.095078</td>\n",
       "      <td>0.147084</td>\n",
       "      <td>0.249766</td>\n",
       "      <td>0.051622</td>\n",
       "      <td>0.064697</td>\n",
       "      <td>0.084963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>347.347634</td>\n",
       "      <td>0.095715</td>\n",
       "      <td>0.148320</td>\n",
       "      <td>0.250197</td>\n",
       "      <td>0.051966</td>\n",
       "      <td>0.065179</td>\n",
       "      <td>0.085298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>348.231507</td>\n",
       "      <td>0.096689</td>\n",
       "      <td>0.150193</td>\n",
       "      <td>0.251582</td>\n",
       "      <td>0.052310</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>346.390150</td>\n",
       "      <td>0.097644</td>\n",
       "      <td>0.150305</td>\n",
       "      <td>0.253399</td>\n",
       "      <td>0.052674</td>\n",
       "      <td>0.065872</td>\n",
       "      <td>0.086215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.128390</td>\n",
       "      <td>345.166613</td>\n",
       "      <td>0.098094</td>\n",
       "      <td>0.150998</td>\n",
       "      <td>0.254710</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.066328</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss    time_sec    Hit@10    Hit@20    Hit@50   NDCG@10  \\\n",
       "30     31  0.143408  343.682516  0.091726  0.142908  0.243099  0.049546   \n",
       "31     32  0.141633  344.392361  0.092400  0.143376  0.244372  0.049864   \n",
       "32     33  0.139377  345.608465  0.093505  0.144519  0.245889  0.050496   \n",
       "33     34  0.137745  343.653309  0.093955  0.145511  0.247013  0.050797   \n",
       "34     35  0.136299  346.263726  0.094517  0.146391  0.248118  0.051158   \n",
       "35     36  0.134399  347.004332  0.095078  0.147084  0.249766  0.051622   \n",
       "36     37  0.133310  347.347634  0.095715  0.148320  0.250197  0.051966   \n",
       "37     38  0.131222  348.231507  0.096689  0.150193  0.251582  0.052310   \n",
       "38     39  0.130075  346.390150  0.097644  0.150305  0.253399  0.052674   \n",
       "39     40  0.128390  345.166613  0.098094  0.150998  0.254710  0.053054   \n",
       "\n",
       "     NDCG@20   NDCG@50  \n",
       "30  0.062406  0.082171  \n",
       "31  0.062682  0.082625  \n",
       "32  0.063297  0.083307  \n",
       "33  0.063747  0.083777  \n",
       "34  0.064186  0.084266  \n",
       "35  0.064697  0.084963  \n",
       "36  0.065179  0.085298  \n",
       "37  0.065714  0.085714  \n",
       "38  0.065872  0.086215  \n",
       "39  0.066328  0.086800  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best epoch: 40 | best val NDCG@10: 0.053054\n",
      "\n",
      "TEST metrics (Graph3-STABLE, best epoch):\n",
      "  Hit@10: 0.097213\n",
      "  Hit@20: 0.147496\n",
      "  Hit@50: 0.254785\n",
      "  NDCG@10: 0.052499\n",
      "  NDCG@20: 0.065097\n",
      "  NDCG@50: 0.086235\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Train LightGCN on Graph3 — propagate per update step (most stable)\n",
    "# - No caching of all_emb across many backward calls\n",
    "# - No retain_graph needed\n",
    "\n",
    "import time, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "EMB_DIM = 64\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "ACC_STEPS = 16\n",
    "EPOCHS = 40\n",
    "\n",
    "USER_BATCH_EVAL = 1024\n",
    "PATIENCE = 6\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Training config:\", {\n",
    "    \"EMB_DIM\": EMB_DIM, \"NUM_LAYERS\": NUM_LAYERS,\n",
    "    \"LR\": LR, \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE, \"ACC_STEPS\": ACC_STEPS,\n",
    "    \"EPOCHS\": EPOCHS, \"USER_BATCH_EVAL\": USER_BATCH_EVAL,\n",
    "    \"PATIENCE\": PATIENCE, \"MIN_DELTA\": MIN_DELTA\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.normal_(self.emb.weight, std=0.01)\n",
    "\n",
    "    def propagate(self, A_norm):\n",
    "        x0 = self.emb.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        for _ in range(self.num_layers):\n",
    "            x = torch.sparse.mm(A_norm, x)\n",
    "            out = out + x\n",
    "        return out / (self.num_layers + 1)\n",
    "\n",
    "def bpr_loss(u, p, n):\n",
    "    pos = (u * p).sum(dim=1)\n",
    "    neg = (u * n).sum(dim=1)\n",
    "    return -torch.log(torch.sigmoid(pos - neg) + 1e-12).mean()\n",
    "\n",
    "model = LightGCN(num_nodes=num_nodes, emb_dim=EMB_DIM, num_layers=NUM_LAYERS).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "A_norm = A_norm.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Data\n",
    "# -------------------------\n",
    "train_users = train_ui[:, 0].astype(np.int32)\n",
    "train_pos_items = train_ui[:, 1].astype(np.int32)\n",
    "n_train = len(train_users)\n",
    "n_batches = math.ceil(n_train / BATCH_SIZE)\n",
    "\n",
    "def sample_negatives(users_np: np.ndarray, n_try: int = 8) -> np.ndarray:\n",
    "    neg = np.random.randint(0, B, size=len(users_np), dtype=np.int32)\n",
    "    for _ in range(n_try):\n",
    "        bad = np.zeros(len(users_np), dtype=bool)\n",
    "        for i, u in enumerate(users_np):\n",
    "            seen = train_pos.get(int(u), None)\n",
    "            if seen and int(neg[i]) in seen:\n",
    "                bad[i] = True\n",
    "        if not bad.any():\n",
    "            return neg\n",
    "        neg[bad] = np.random.randint(0, B, size=bad.sum(), dtype=np.int32)\n",
    "    return neg\n",
    "\n",
    "# -------------------------\n",
    "# Train + early stop\n",
    "# -------------------------\n",
    "history = []\n",
    "best_ndcg10, best_epoch = -1.0, -1\n",
    "best_state = None\n",
    "pat = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "\n",
    "    perm = np.random.permutation(n_train)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for bi in range(n_batches):\n",
    "        sl = slice(bi * BATCH_SIZE, min((bi + 1) * BATCH_SIZE, n_train))\n",
    "        idx = perm[sl]\n",
    "\n",
    "        u = train_users[idx]\n",
    "        p = train_pos_items[idx]\n",
    "        n = sample_negatives(u)\n",
    "\n",
    "        u_t = torch.from_numpy(u.astype(np.int64)).to(DEVICE) + user_offset\n",
    "        p_t = torch.from_numpy(p.astype(np.int64)).to(DEVICE) + book_offset\n",
    "        n_t = torch.from_numpy(n.astype(np.int64)).to(DEVICE) + book_offset\n",
    "\n",
    "        # ключ: propagate под каждый шаг (граф уникальный -> backward безопасен)\n",
    "        all_emb = model.propagate(A_norm)\n",
    "\n",
    "        loss = bpr_loss(all_emb[u_t], all_emb[p_t], all_emb[n_t]) / ACC_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += float(loss.item()) * ACC_STEPS\n",
    "\n",
    "        if (bi + 1) % ACC_STEPS == 0 or (bi + 1) == n_batches:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ---- Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_emb_eval = model.propagate(A_norm)\n",
    "\n",
    "    val_metrics = eval_ranking(\n",
    "        all_emb_eval, val_gt, train_pos,\n",
    "        K_list=(10, 20, 50),\n",
    "        user_batch_size=USER_BATCH_EVAL,\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    row = {\"epoch\": epoch, \"loss\": epoch_loss / max(n_batches, 1), \"time_sec\": dt, **val_metrics}\n",
    "    history.append(row)\n",
    "\n",
    "    print(\n",
    "        f\"[Graph3-STABLE] Epoch {epoch:02d} | loss={row['loss']:.4f} | time={dt:.1f}s | \"\n",
    "        f\"Hit@10={val_metrics['Hit@10']:.4f} NDCG@10={val_metrics['NDCG@10']:.4f} | \"\n",
    "        f\"Hit@50={val_metrics['Hit@50']:.4f} NDCG@50={val_metrics['NDCG@50']:.4f}\"\n",
    "    )\n",
    "\n",
    "    ndcg10 = float(val_metrics[\"NDCG@10\"])\n",
    "    if ndcg10 > best_ndcg10 + MIN_DELTA:\n",
    "        best_ndcg10 = ndcg10\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        pat = 0\n",
    "        print(f\"New best NDCG@10={best_ndcg10:.5f} at epoch {best_epoch}\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        print(f\"  patience={pat}/{PATIENCE} (best {best_ndcg10:.5f} @ epoch {best_epoch})\")\n",
    "        if pat >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# restore best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "display(hist_df.tail(10))\n",
    "print(f\"\\nBest epoch: {best_epoch} | best val NDCG@10: {best_ndcg10:.6f}\")\n",
    "\n",
    "# ---- TEST\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_emb_test = model.propagate(A_norm)\n",
    "\n",
    "test_metrics = eval_ranking(\n",
    "    all_emb_test, test_gt, train_pos,\n",
    "    K_list=(10, 20, 50),\n",
    "    user_batch_size=USER_BATCH_EVAL,\n",
    ")\n",
    "\n",
    "print(\"\\nTEST metrics (Graph3-STABLE, best epoch):\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6f178-c282-4c6c-b7c4-9f0f0864666d",
   "metadata": {},
   "source": [
    "Graph3: Enriched Graph Recommender (LightGCN)\n",
    "\n",
    "Goal.\n",
    "Move beyond a pure user–book bipartite graph and test whether adding structured content + metadata relations improves ranking quality for book recommendation on Goodbooks-10k.\n",
    "\n",
    "Graph construction (Graph3).\n",
    "We built a single unified node index space and added multiple edge types:\n",
    "\n",
    "user ↔ book (train interactions only; val/test edges excluded to avoid leakage)\n",
    "\n",
    "book ↔ tag (weighted edges using log1p(count); filtered by MIN_BOOK_FREQ and top-K tags per book)\n",
    "\n",
    "book ↔ book similarity edges (content-based similarity computed from book text; top-K neighbors per book)\n",
    "\n",
    "book ↔ author (author nodes extracted from book metadata)\n",
    "\n",
    "book ↔ language (language_code nodes)\n",
    "\n",
    "book ↔ year_bin (binned publication year nodes)\n",
    "\n",
    "All edges were made bidirectional (undirected) and combined into a single sparse adjacency matrix, then normalized (A_norm) for LightGCN propagation.\n",
    "\n",
    "Model.\n",
    "We trained a LightGCN encoder (3 propagation layers) on the enriched graph using BPR loss with negative sampling (filtering seen train positives). Validation and test used leave-one-out ranking with metrics Hit@K and NDCG@K.\n",
    "\n",
    "Key results.\n",
    "Training was stable and metrics improved steadily up to epoch 40.\n",
    "\n",
    "Best validation (epoch 40):\n",
    "\n",
    "Hit@10 = 0.0981\n",
    "\n",
    "NDCG@10 = 0.05305\n",
    "\n",
    "Hit@50 = 0.2547\n",
    "\n",
    "NDCG@50 = 0.0868\n",
    "\n",
    "Test (best epoch):\n",
    "\n",
    "Hit@10 = 0.09721\n",
    "\n",
    "NDCG@10 = 0.05250\n",
    "\n",
    "Hit@20 = 0.14750\n",
    "\n",
    "NDCG@20 = 0.06510\n",
    "\n",
    "Hit@50 = 0.25479\n",
    "\n",
    "NDCG@50 = 0.08624\n",
    "\n",
    "Takeaways.\n",
    "\n",
    "Adding heterogeneous, meaningful relations (tags + metadata + similarity) significantly improves ranking quality compared to the simpler graphs tested earlier.\n",
    "\n",
    "Validation ≈ Test indicates the gains generalize and are not just overfitting to validation.\n",
    "\n",
    "Even with a simple homogeneous LightGCN, the enriched graph provides a strong boost — suggesting the next step is to use heterogeneous GNNs that can treat edge types differently (rather than averaging everything equally).\n",
    "\n",
    "Next steps.\n",
    "\n",
    "Run ablation studies (remove one relation group at a time) to quantify which edges drive the improvement (tags vs authors vs similarity vs language/year).\n",
    "\n",
    "Upgrade the model family to heterogeneous architectures (e.g., R-GCN / HGT / HAN / HeteroConv) to leverage edge-type structure more effectively.\n",
    "\n",
    "Improve the content layer by incorporating stronger text embeddings (e.g., SBERT) either as node features or similarity edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ac7eff-8931-47aa-b2a9-ae2c7a098b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuild target: U=53398 B=9999 T=5014 num_nodes=74285\n",
      "Offsets: {'user': 0, 'book': 53398, 'tag': 63397, 'author': 68411, 'lang': 74252, 'year': 74278}\n",
      "[OK] added user-book edges\n",
      "[OK] added book-tag edges\n",
      "[OK] added book-book sim edges via TF-IDF: 265581\n",
      "[OK] added book-author edges: 13215\n",
      "[OK] added book-lang edges: 9999\n",
      "[OK] added book-year edges: 9999\n",
      "\n",
      "Rebuilt:\n",
      " edge_index: (2, 11450076)\n",
      " edge_w: (11450076,) | finite: True\n",
      " edge_type: (11450076,) | rels: {'user_book': 0, 'book_user': 1, 'book_tag': 2, 'tag_book': 3, 'book_book_sim': 4, 'book_author': 5, 'author_book': 6, 'book_lang': 7, 'lang_book': 8, 'book_year': 9, 'year_book': 10}\n",
      " A_norm: (74285, 74285) nnz: 11260518\n",
      "\n",
      "[OK] edge_type + rel2id are ready ✔\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell: Rebuild Graph3 edges WITH relation types (edge_type/rel2id)\n",
    "# - produces: edge_index, edge_w, edge_type, rel2id\n",
    "# - optionally rebuilds: A_norm\n",
    "# ============================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def ensure_tensor(x, dtype, device=\"cpu\"):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device=device, dtype=dtype)\n",
    "    return torch.tensor(x, dtype=dtype, device=device)\n",
    "\n",
    "def add_edges(store, src, dst, w, rel_name, rel2id):\n",
    "    \"\"\"\n",
    "    store: dict with lists\n",
    "    src/dst: 1D Long tensor or array-like\n",
    "    w: 1D float tensor or array-like (same length)\n",
    "    rel_name: str\n",
    "    \"\"\"\n",
    "    if rel_name not in rel2id:\n",
    "        rel2id[rel_name] = len(rel2id)\n",
    "    rid = rel2id[rel_name]\n",
    "\n",
    "    src = ensure_tensor(src, torch.long, device=\"cpu\").view(-1)\n",
    "    dst = ensure_tensor(dst, torch.long, device=\"cpu\").view(-1)\n",
    "    w   = ensure_tensor(w, torch.float32, device=\"cpu\").view(-1)\n",
    "\n",
    "    assert src.numel() == dst.numel() == w.numel(), f\"{rel_name}: src/dst/w size mismatch\"\n",
    "\n",
    "    store[\"src\"].append(src)\n",
    "    store[\"dst\"].append(dst)\n",
    "    store[\"w\"].append(w)\n",
    "    store[\"t\"].append(torch.full((src.numel(),), rid, dtype=torch.int16))\n",
    "\n",
    "def build_sparse_norm(edge_index, edge_w, num_nodes):\n",
    "    \"\"\"\n",
    "    Weighted LightGCN normalization: D^{-1/2} A D^{-1/2}\n",
    "    where A is undirected (we already add both directions)\n",
    "    \"\"\"\n",
    "    # COO\n",
    "    row = edge_index[0]\n",
    "    col = edge_index[1]\n",
    "    val = edge_w\n",
    "\n",
    "    # degree = sum of weights on outgoing edges\n",
    "    deg = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "    deg.scatter_add_(0, row, val)\n",
    "    deg = torch.clamp(deg, min=1e-12)\n",
    "\n",
    "    inv_sqrt = torch.pow(deg, -0.5)\n",
    "    norm_val = inv_sqrt[row] * val * inv_sqrt[col]\n",
    "\n",
    "    A = torch.sparse_coo_tensor(\n",
    "        indices=edge_index,\n",
    "        values=norm_val,\n",
    "        size=(num_nodes, num_nodes),\n",
    "        dtype=torch.float32\n",
    "    ).coalesce()\n",
    "    return A\n",
    "\n",
    "# ----------------------------\n",
    "# Preconditions\n",
    "# ----------------------------\n",
    "need = [\"U\",\"B\",\"T\",\"num_nodes\",\n",
    "        \"user_offset\",\"book_offset\",\"tag_offset\",\"author_offset\",\"lang_offset\",\"year_offset\",\n",
    "        \"train_ui\",\"books_df\",\"bt\",\n",
    "        \"author2idx\",\"lang2idx\",\"year2idx\"]\n",
    "missing = [v for v in need if v not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required variables for rebuild: {missing}\")\n",
    "\n",
    "U_cnt, B_cnt, T_cnt = int(U), int(B), int(T)\n",
    "num_nodes_cnt = int(num_nodes)\n",
    "\n",
    "print(\"Rebuild target:\",\n",
    "      f\"U={U_cnt} B={B_cnt} T={T_cnt} num_nodes={num_nodes_cnt}\")\n",
    "print(\"Offsets:\",\n",
    "      {\"user\": int(user_offset), \"book\": int(book_offset), \"tag\": int(tag_offset),\n",
    "       \"author\": int(author_offset), \"lang\": int(lang_offset), \"year\": int(year_offset)})\n",
    "\n",
    "# ----------------------------\n",
    "# Start building\n",
    "# ----------------------------\n",
    "rel2id = {}\n",
    "store = {\"src\": [], \"dst\": [], \"w\": [], \"t\": []}\n",
    "\n",
    "# ---- 1) user-book (TRAIN only), weight=1, bidirectional\n",
    "train_u = train_ui[:, 0].astype(np.int64)\n",
    "train_b = train_ui[:, 1].astype(np.int64)\n",
    "\n",
    "src = torch.from_numpy(train_u) + int(user_offset)\n",
    "dst = torch.from_numpy(train_b) + int(book_offset)\n",
    "w1  = torch.ones(src.numel(), dtype=torch.float32)\n",
    "\n",
    "add_edges(store, src, dst, w1, \"user_book\", rel2id)\n",
    "add_edges(store, dst, src, w1, \"book_user\", rel2id)\n",
    "print(\"[OK] added user-book edges\")\n",
    "\n",
    "# ---- 2) book-tag edges (from bt), bidirectional\n",
    "# bt must contain: book_idx, tag_idx, w\n",
    "assert {\"book_idx\",\"tag_idx\",\"w\"}.issubset(bt.columns), f\"bt columns missing, got {bt.columns.tolist()}\"\n",
    "bt_books = bt[\"book_idx\"].to_numpy(dtype=np.int64)\n",
    "bt_tags  = bt[\"tag_idx\"].to_numpy(dtype=np.int64)\n",
    "bt_w     = bt[\"w\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "src_bt = torch.from_numpy(bt_books) + int(book_offset)\n",
    "dst_bt = torch.from_numpy(bt_tags) + int(tag_offset)\n",
    "w_bt   = torch.from_numpy(bt_w)\n",
    "\n",
    "add_edges(store, src_bt, dst_bt, w_bt, \"book_tag\", rel2id)\n",
    "add_edges(store, dst_bt, src_bt, w_bt, \"tag_book\", rel2id)\n",
    "print(\"[OK] added book-tag edges\")\n",
    "\n",
    "# ---- 3) book-book similarity edges (optional)\n",
    "# Prefer using existing sim dataframe if present, else recompute\n",
    "sim_df = None\n",
    "for cand in [\"sim_df\", \"bb_sim_df\", \"book_book_sim_df\", \"book_sim_df\"]:\n",
    "    if cand in globals():\n",
    "        sim_df = globals()[cand]\n",
    "        break\n",
    "\n",
    "if sim_df is not None:\n",
    "    # expected columns: i (book_idx), j (book_idx), sim (float)\n",
    "    cols = set(sim_df.columns)\n",
    "    if {\"i\",\"j\",\"sim\"}.issubset(cols):\n",
    "        i = sim_df[\"i\"].to_numpy(np.int64)\n",
    "        j = sim_df[\"j\"].to_numpy(np.int64)\n",
    "        s = sim_df[\"sim\"].to_numpy(np.float32)\n",
    "    elif {\"book_i\",\"book_j\",\"sim\"}.issubset(cols):\n",
    "        i = sim_df[\"book_i\"].to_numpy(np.int64)\n",
    "        j = sim_df[\"book_j\"].to_numpy(np.int64)\n",
    "        s = sim_df[\"sim\"].to_numpy(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sim_df schema: {sim_df.columns.tolist()}\")\n",
    "\n",
    "    src_bb = torch.from_numpy(i) + int(book_offset)\n",
    "    dst_bb = torch.from_numpy(j) + int(book_offset)\n",
    "    w_bb   = torch.from_numpy(s)\n",
    "\n",
    "    add_edges(store, src_bb, dst_bb, w_bb, \"book_book_sim\", rel2id)\n",
    "    add_edges(store, dst_bb, src_bb, w_bb, \"book_book_sim\", rel2id)\n",
    "    print(\"[OK] added book-book sim edges from existing sim_df:\", len(s))\n",
    "else:\n",
    "    # Fallback: recompute TF-IDF cosine topk\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    TOPK_SIM = int(globals().get(\"TOPK_SIM\", 30))\n",
    "    MIN_SIM  = float(globals().get(\"MIN_SIM\", 0.2))\n",
    "    TFIDF_MIN_DF = int(globals().get(\"TFIDF_MIN_DF\", 2))\n",
    "    TFIDF_NGRAMS = tuple(globals().get(\"TFIDF_NGRAMS\", (1,2)))\n",
    "\n",
    "    texts = books_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "    tfidf = TfidfVectorizer(min_df=TFIDF_MIN_DF, ngram_range=TFIDF_NGRAMS)\n",
    "    X = tfidf.fit_transform(texts)\n",
    "\n",
    "    # cosine sim row-by-row topk \n",
    "    rows_i, rows_j, sims = [], [], []\n",
    "    for i in range(X.shape[0]):\n",
    "        # sim to all\n",
    "        sim = cosine_similarity(X[i], X).ravel()\n",
    "        sim[i] = 0.0\n",
    "        # topk\n",
    "        idx = np.argpartition(-sim, TOPK_SIM)[:TOPK_SIM]\n",
    "        idx = idx[sim[idx] >= MIN_SIM]\n",
    "        for j in idx:\n",
    "            rows_i.append(i); rows_j.append(int(j)); sims.append(float(sim[j]))\n",
    "\n",
    "    i = np.array(rows_i, dtype=np.int64)\n",
    "    j = np.array(rows_j, dtype=np.int64)\n",
    "    s = np.array(sims, dtype=np.float32)\n",
    "\n",
    "    src_bb = torch.from_numpy(i) + int(book_offset)\n",
    "    dst_bb = torch.from_numpy(j) + int(book_offset)\n",
    "    w_bb   = torch.from_numpy(s)\n",
    "\n",
    "    add_edges(store, src_bb, dst_bb, w_bb, \"book_book_sim\", rel2id)\n",
    "    add_edges(store, dst_bb, src_bb, w_bb, \"book_book_sim\", rel2id)\n",
    "    print(\"[OK] added book-book sim edges via TF-IDF:\", len(s))\n",
    "\n",
    "# ---- 4) book-author edges (bidirectional)\n",
    "# books_df must contain 'authors' and 'book_idx'\n",
    "if \"authors\" not in books_df.columns:\n",
    "    raise NameError(\"books_df must have 'authors' column for book-author edges\")\n",
    "\n",
    "def parse_authors(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    # goodbooks: \"Suzanne Collins\" (sometimes multiple authors separated by ',')\n",
    "    parts = [p.strip() for p in str(x).split(\",\")]\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "ba_book = []\n",
    "ba_auth = []\n",
    "for _, r in books_df[[\"book_idx\",\"authors\"]].iterrows():\n",
    "    bidx = int(r[\"book_idx\"])\n",
    "    for a in parse_authors(r[\"authors\"]):\n",
    "        if a in author2idx:\n",
    "            ba_book.append(bidx)\n",
    "            ba_auth.append(int(author2idx[a]))\n",
    "\n",
    "ba_book = np.array(ba_book, dtype=np.int64)\n",
    "ba_auth = np.array(ba_auth, dtype=np.int64)\n",
    "\n",
    "src_ba = torch.from_numpy(ba_book) + int(book_offset)\n",
    "dst_ba = torch.from_numpy(ba_auth) + int(author_offset)\n",
    "w_ba   = torch.ones(src_ba.numel(), dtype=torch.float32)\n",
    "\n",
    "add_edges(store, src_ba, dst_ba, w_ba, \"book_author\", rel2id)\n",
    "add_edges(store, dst_ba, src_ba, w_ba, \"author_book\", rel2id)\n",
    "print(\"[OK] added book-author edges:\", len(ba_book))\n",
    "\n",
    "# ---- 5) book-lang edges (bidirectional)\n",
    "if \"language_code\" not in books_df.columns:\n",
    "    raise NameError(\"books_df must have 'language_code' column for book-lang edges\")\n",
    "\n",
    "bl_book = []\n",
    "bl_lang = []\n",
    "for _, r in books_df[[\"book_idx\",\"language_code\"]].iterrows():\n",
    "    bidx = int(r[\"book_idx\"])\n",
    "    lang = r[\"language_code\"]\n",
    "    if pd.isna(lang): \n",
    "        continue\n",
    "    lang = str(lang)\n",
    "    if lang in lang2idx:\n",
    "        bl_book.append(bidx)\n",
    "        bl_lang.append(int(lang2idx[lang]))\n",
    "\n",
    "bl_book = np.array(bl_book, dtype=np.int64)\n",
    "bl_lang = np.array(bl_lang, dtype=np.int64)\n",
    "\n",
    "src_bl = torch.from_numpy(bl_book) + int(book_offset)\n",
    "dst_bl = torch.from_numpy(bl_lang) + int(lang_offset)\n",
    "w_bl   = torch.ones(src_bl.numel(), dtype=torch.float32)\n",
    "\n",
    "add_edges(store, src_bl, dst_bl, w_bl, \"book_lang\", rel2id)\n",
    "add_edges(store, dst_bl, src_bl, w_bl, \"lang_book\", rel2id)\n",
    "print(\"[OK] added book-lang edges:\", len(bl_book))\n",
    "\n",
    "# ---- 6) book-yearbin edges (bidirectional)\n",
    "if \"year_bin\" not in books_df.columns:\n",
    "    raise NameError(\"books_df must have 'year_bin' column for book-year edges\")\n",
    "\n",
    "by_book = []\n",
    "by_year = []\n",
    "for _, r in books_df[[\"book_idx\",\"year_bin\"]].iterrows():\n",
    "    bidx = int(r[\"book_idx\"])\n",
    "    yb = r[\"year_bin\"]\n",
    "    if pd.isna(yb):\n",
    "        continue\n",
    "    yb = str(yb)\n",
    "    if yb in year2idx:\n",
    "        by_book.append(bidx)\n",
    "        by_year.append(int(year2idx[yb]))\n",
    "\n",
    "by_book = np.array(by_book, dtype=np.int64)\n",
    "by_year = np.array(by_year, dtype=np.int64)\n",
    "\n",
    "src_by = torch.from_numpy(by_book) + int(book_offset)\n",
    "dst_by = torch.from_numpy(by_year) + int(year_offset)\n",
    "w_by   = torch.ones(src_by.numel(), dtype=torch.float32)\n",
    "\n",
    "add_edges(store, src_by, dst_by, w_by, \"book_year\", rel2id)\n",
    "add_edges(store, dst_by, src_by, w_by, \"year_book\", rel2id)\n",
    "print(\"[OK] added book-year edges:\", len(by_book))\n",
    "\n",
    "# ----------------------------\n",
    "# Final concat\n",
    "# ----------------------------\n",
    "edge_src = torch.cat(store[\"src\"], dim=0)\n",
    "edge_dst = torch.cat(store[\"dst\"], dim=0)\n",
    "edge_w   = torch.cat(store[\"w\"], dim=0).float()\n",
    "edge_type= torch.cat(store[\"t\"], dim=0)\n",
    "\n",
    "edge_index = torch.stack([edge_src, edge_dst], dim=0).long()\n",
    "\n",
    "print(\"\\nRebuilt:\")\n",
    "print(\" edge_index:\", tuple(edge_index.shape))\n",
    "print(\" edge_w:\", tuple(edge_w.shape), \"| finite:\", bool(torch.isfinite(edge_w).all().item()))\n",
    "print(\" edge_type:\", tuple(edge_type.shape), \"| rels:\", rel2id)\n",
    "\n",
    "# sanity\n",
    "assert int(edge_index.min()) >= 0\n",
    "assert int(edge_index.max()) < int(num_nodes_cnt)\n",
    "assert edge_w.numel() == edge_type.numel() == edge_index.shape[1]\n",
    "\n",
    "# ----------------------------\n",
    "# (Optional) rebuild A_norm for consistency in bundle\n",
    "# ----------------------------\n",
    "REBUILD_A_NORM = True\n",
    "if REBUILD_A_NORM:\n",
    "    A_norm = build_sparse_norm(edge_index, edge_w, num_nodes_cnt)\n",
    "    print(\" A_norm:\", tuple(A_norm.shape), \"nnz:\", int(A_norm._nnz()))\n",
    "\n",
    "print(\"\\n[OK] edge_type + rel2id are ready ✔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b07ffe0-42c1-4aec-a036-5dced72f8ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\n",
      "[OK] saved: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\\graph3_state.pt\n",
      "[OK] saved: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\\splits_ui.npz\n",
      "[OK] saved mappings: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\\mappings\n",
      "[OK] saved meta/config: D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\n",
      "Bundle ready ✅ D:\\ML\\GNN\\graph_recsys\\artifacts\\v2_proper\\graph3_bundle\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# SAVE Graph3 bundle (final) — with edge_type + rel2id\n",
    "# ============================\n",
    "\n",
    "# ---------- Paths ----------\n",
    "PROJECT_ROOT = Path(r\"D:/ML/GNN/graph_recsys\")\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\" / \"v2_proper\"\n",
    "ARTIFACTS = PROJECT_ROOT / \"artifacts\" / \"v2_proper\"\n",
    "\n",
    "BUNDLE_DIR = ARTIFACTS / \"graph3_bundle\"\n",
    "MAP_DIR = BUNDLE_DIR / \"mappings\"\n",
    "BUNDLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving to:\", BUNDLE_DIR)\n",
    "\n",
    "# ---------- Guardrails: required objects ----------\n",
    "required = [\n",
    "    \"A_norm\", \"edge_index\", \"edge_w\", \"edge_type\", \"rel2id\",\n",
    "    \"train_ui\", \"val_ui\", \"test_ui\",\n",
    "    \"user2idx\", \"book2idx\",\n",
    "    \"U\", \"B\", \"T\",\n",
    "    \"user_offset\", \"book_offset\", \"tag_offset\",\n",
    "    \"author_offset\", \"lang_offset\", \"year_offset\",\n",
    "    \"tag2idx\", \"author2idx\", \"lang2idx\", \"year2idx\",\n",
    "    \"num_nodes\",\n",
    "]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing variables in notebook scope: {missing}\")\n",
    "\n",
    "# ---------- Counts ----------\n",
    "U_cnt = int(U)\n",
    "B_cnt = int(B)\n",
    "T_cnt = int(T)\n",
    "A_cnt = int(len(author2idx))\n",
    "L_cnt = int(len(lang2idx))\n",
    "Y_cnt = int(len(year2idx))\n",
    "\n",
    "# ---------- 1) Save graph tensors (CPU) ----------\n",
    "A_norm_cpu = A_norm.coalesce().to(\"cpu\")\n",
    "\n",
    "edge_index_cpu = edge_index.to(\"cpu\")\n",
    "edge_w_cpu = edge_w.to(\"cpu\")\n",
    "edge_type_cpu = edge_type.to(\"cpu\")\n",
    "\n",
    "graph_state = {\n",
    "    # sparse normalized adjacency for full-batch LightGCN\n",
    "    \"A_norm\": A_norm_cpu,                  # torch sparse COO [num_nodes, num_nodes]\n",
    "    \"num_nodes\": int(num_nodes),\n",
    "\n",
    "    # counts (per node type in unified index space)\n",
    "    \"U\": U_cnt,\n",
    "    \"B\": B_cnt,\n",
    "    \"T\": T_cnt,\n",
    "    \"A_authors\": A_cnt,\n",
    "    \"L\": L_cnt,\n",
    "    \"Y\": Y_cnt,\n",
    "\n",
    "    # offsets\n",
    "    \"offsets\": {\n",
    "        \"user_offset\": int(user_offset),\n",
    "        \"book_offset\": int(book_offset),\n",
    "        \"tag_offset\": int(tag_offset),\n",
    "        \"author_offset\": int(author_offset),\n",
    "        \"lang_offset\": int(lang_offset),\n",
    "        \"year_offset\": int(year_offset),\n",
    "    },\n",
    "\n",
    "    # vocabularies (node-local ids for each type)\n",
    "    \"tag2idx\": tag2idx,\n",
    "    \"author2idx\": author2idx,\n",
    "    \"lang2idx\": lang2idx,\n",
    "    \"year2idx\": year2idx,\n",
    "\n",
    "    # raw edges (useful for hetero models / typed message passing)\n",
    "    \"edge_index\": edge_index_cpu,          # [2, E]\n",
    "    \"edge_w\": edge_w_cpu,                  # [E]\n",
    "    \"edge_type\": edge_type_cpu,            # [E] int rel ids\n",
    "    \"rel2id\": rel2id,                      # dict[str,int]\n",
    "}\n",
    "\n",
    "torch.save(graph_state, BUNDLE_DIR / \"graph3_state.pt\")\n",
    "print(\"[OK] saved:\", BUNDLE_DIR / \"graph3_state.pt\")\n",
    "\n",
    "# ---------- 2) Save splits ----------\n",
    "np.savez_compressed(\n",
    "    BUNDLE_DIR / \"splits_ui.npz\",\n",
    "    train_ui=train_ui.astype(np.int64),\n",
    "    val_ui=val_ui.astype(np.int64),\n",
    "    test_ui=test_ui.astype(np.int64),\n",
    "    U=U_cnt,\n",
    "    B=B_cnt,\n",
    ")\n",
    "print(\"[OK] saved:\", BUNDLE_DIR / \"splits_ui.npz\")\n",
    "\n",
    "# ---------- 3) Save mappings (dict -> Series CSV) ----------\n",
    "pd.Series(user2idx).to_csv(MAP_DIR / \"user2idx.csv\", header=False)\n",
    "pd.Series(book2idx).to_csv(MAP_DIR / \"book2idx.csv\", header=False)\n",
    "\n",
    "idx2user = {v: k for k, v in user2idx.items()}\n",
    "idx2book = {v: k for k, v in book2idx.items()}\n",
    "pd.Series(idx2user).to_csv(MAP_DIR / \"idx2user.csv\", header=False)\n",
    "pd.Series(idx2book).to_csv(MAP_DIR / \"idx2book.csv\", header=False)\n",
    "print(\"[OK] saved mappings:\", MAP_DIR)\n",
    "\n",
    "# ---------- 4) Save meta/config ----------\n",
    "# These may or may not exist depending on your cells; handle safely.\n",
    "build_config = dict(\n",
    "    graph_name=\"Graph3\",\n",
    "    notes=(\n",
    "        \"Unified graph: user-book(train only) + book-tag(log1p count) + \"\n",
    "        \"book-book tfidf cosine + book-author + book-lang + book-year_bin\"\n",
    "    ),\n",
    "    MIN_BOOK_FREQ=int(globals().get(\"MIN_BOOK_FREQ\", -1)),\n",
    "    TOP_TAGS_PER_BOOK=int(globals().get(\"TOP_TAGS_PER_BOOK\", -1)),\n",
    "    TOPK_SIM=int(globals().get(\"TOPK_SIM\", -1)),\n",
    "    MIN_SIM=float(globals().get(\"MIN_SIM\", -1.0)),\n",
    "    tfidf_min_df=int(globals().get(\"TFIDF_MIN_DF\", 2)),\n",
    "    tfidf_ngrams=list(globals().get(\"TFIDF_NGRAMS\", [1, 2])),\n",
    ")\n",
    "\n",
    "meta = dict(\n",
    "    created_at_utc=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    counts=dict(U=U_cnt, B=B_cnt, T=T_cnt, A_authors=A_cnt, L=L_cnt, Y=Y_cnt),\n",
    "    num_nodes=int(num_nodes),\n",
    "    nnz=int(A_norm_cpu._nnz()),\n",
    "    E=int(edge_index_cpu.shape[1]),\n",
    "    rels=int(len(rel2id)),\n",
    "    train_edges=int(train_ui.shape[0]),\n",
    "    val_edges=int(val_ui.shape[0]),\n",
    "    test_edges=int(test_ui.shape[0]),\n",
    "    edge_index_shape=list(edge_index_cpu.shape),\n",
    "    edge_w_shape=list(edge_w_cpu.shape),\n",
    "    edge_type_shape=list(edge_type_cpu.shape),\n",
    "    offsets=graph_state[\"offsets\"],\n",
    ")\n",
    "\n",
    "(BUNDLE_DIR / \"build_config.json\").write_text(\n",
    "    json.dumps(build_config, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    ")\n",
    "(BUNDLE_DIR / \"meta.json\").write_text(\n",
    "    json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"[OK] saved meta/config:\", BUNDLE_DIR)\n",
    "print(\"Bundle ready ✅\", BUNDLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf8b1f51-7bbf-44bd-9a06-7b08b9223516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\555\\AppData\\Local\\Temp\\ipykernel_19020\\1501791894.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(bundle_dir / \"graph3_state.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm: torch.Size([74285, 74285]) nnz: 11260518\n",
      "edge_index: torch.Size([2, 11450076]) edge_w: torch.Size([11450076]) edge_type: torch.Size([11450076])\n",
      "rels: {'user_book': 0, 'book_user': 1, 'book_tag': 2, 'tag_book': 3, 'book_book_sim': 4, 'book_author': 5, 'author_book': 6, 'book_lang': 7, 'lang_book': 8, 'book_year': 9, 'year_book': 10}\n",
      "U,B: 53398 9999 train: (4926384, 2) val: (53398, 2) test: (53398, 2)\n",
      "[OK] bundle sanity passed ✅\n"
     ]
    }
   ],
   "source": [
    "bundle_dir = Path(r\"D:/ML/GNN/graph_recsys/artifacts/v2_proper/graph3_bundle\")\n",
    "g = torch.load(bundle_dir / \"graph3_state.pt\", map_location=\"cpu\")\n",
    "z = np.load(bundle_dir / \"splits_ui.npz\")\n",
    "\n",
    "print(\"A_norm:\", g[\"A_norm\"].shape, \"nnz:\", g[\"A_norm\"]._nnz())\n",
    "print(\"edge_index:\", g[\"edge_index\"].shape, \"edge_w:\", g[\"edge_w\"].shape, \"edge_type:\", g[\"edge_type\"].shape)\n",
    "print(\"rels:\", g[\"rel2id\"])\n",
    "print(\"U,B:\", z[\"U\"], z[\"B\"], \"train:\", z[\"train_ui\"].shape, \"val:\", z[\"val_ui\"].shape, \"test:\", z[\"test_ui\"].shape)\n",
    "\n",
    "# sanity\n",
    "assert g[\"A_norm\"].shape[0] == g[\"A_norm\"].shape[1] == g[\"num_nodes\"]\n",
    "assert g[\"edge_index\"].shape[1] == g[\"edge_w\"].numel() == g[\"edge_type\"].numel()\n",
    "assert int(g[\"edge_index\"].max()) < int(g[\"num_nodes\"])\n",
    "print(\"[OK] bundle sanity passed ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - GNN (clean)",
   "language": "python",
   "name": "gnn_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
